{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS-GA 1003: Machine Learning and Computational Statistics\n",
    "## Homework 3: SVM and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculating Subgradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 [Subgradients for pointwise maximum of functions] Suppose $f_{1},\\ldots,f_{m}:\\mathbf R^{d}\\to\\mathbf R$ are convex functions, and $$f(x)=\\max_{i=1,\\ldots,,m}f_{i}(x).$$ Let $k$ be any index for which $f_{k}(x)=f(x)$, and choose $g\\in\\partial f_{k}(x)$.[We are using the fact that a convex function on $\\mathbf R^{d}$ has a non-empty subdifferential at all points.] Show that Formula $g\\in\\partial f(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER:__\n",
    "\n",
    "> Let $k$ be any index for which $f_k(x)=f(x)$, and choose $g\\in \\partial f_k(x)$.\n",
    "\n",
    "> $$f(z)\\geq f_k(z)\\geq f_k(x)+g^T(z-x)=f(x)+g^T(z-x)$$\n",
    "\n",
    "> So for $g\\in \\partial f_k(x)$, $g\\in \\partial f(x)$.\n",
    "\n",
    "> For the subdifferiential $\\partial f(x)$, we have $$\\partial f(x)=\\mathbf{Co}\\cup\\{\\nabla f_i(x)|f_i(x)=f(x)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 [Subgradient of hinge loss for linear prediction] Give a subgradient of $$J(w)=\\max\\left\\{ 0,1-yw^{T}x\\right\\}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER:__\n",
    "\n",
    "> $\\partial f(x)$=max{0, $-y_nx_n$}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will see why this loss function has this name.\n",
    "\n",
    "3.1. Show that if $\\{x | w^T x=0\\}$ is a separating hyperplane for a training set $\\mathcal D = ((x_1, y_1), . . . ,(x_n, y_n))$, then the average perceptron loss on D is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER:__\n",
    "\n",
    "> $\\begin{array}{lll}\n",
    " \\frac{\\ell(D)}{n}&=&\\frac{1}{n}\\sum_{i=1}^n max(0,-\\hat{y_i}y_i)\\\\\n",
    " &=&\\frac{1}{n}\\sum_{i=1}^n max(0,-w_ix_iy_i)\\\\\n",
    " \\end{array}$\n",
    " \n",
    "> $\\{x|w^Tx=0\\}$ seperates the training set, \n",
    "\n",
    "> $$\\forall i, w_ix_iy_i>0, \\frac{\\ell(D)}{n}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Let $\\mathcal H$ be the linear hypothesis space consisting of functions $x \\mapsto w^T x$. Consider running\n",
    "SGD to minimize the empirical risk with the perceptron loss. We’ll use the version of SGD in which we cycle through the data points in each epoch. Show that if we use a fixed step size 1 and we terminate when our training loss is 0, then we are exactly doing the Perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER:__\n",
    "\n",
    "> The update rule for w in Stochastic Subgradient Descend is,\n",
    "\n",
    "> $$w_{t} = w_{t-1}-\\partial_w[max(0,-w_{t-1}^Tx_{t-1}y_{t-1})] $$\n",
    "\n",
    ">$$w_{t}=\\begin{cases}\n",
    "w_{t-1}&-w_{t-1}^Tx_{t-1}y_{t-1}<0\\\\\n",
    "w_{t-1}+y_{t-1}x_{t-1}&-w_{t-1}^Tx_{t-1}y_{t-1}\\geq0\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "> And terminates at training loss is 0 means $$\\forall i, -w_{t-1}^Tx_{t-1}y_{t-1}<0$$ \n",
    "\n",
    "> Which is exactly the perceptron algorithm is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3. Suppose the perceptron algorithm returns $w$. Show that $w$ is a linear combination of the input points. That is, we can write $w =\\sum_{i=1}^n \\alpha_ix_i$ for some $\\alpha_1,..., \\alpha_n \\in \\mathbf R$. The $x_i$ for which $\\alpha_i\\neq0$ are called support vectors. Give a characterization of points that are support vectors and not support vectors with regard to what happens with them during the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> The support vectors in the perceptron algorithm are those misclassified during the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Data\n",
    "\n",
    "Load all the data and randomly split it into 1500 training examples and 500 test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prep\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division,print_function\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timing(f):\n",
    "    def wrap(*args):\n",
    "        time1 = time.time()\n",
    "        ret = f(*args)\n",
    "        time2 = time.time()\n",
    "        print('%s function took %0.3f ms' % (f.func_name, (time2-time1)*1000.0))\n",
    "        return ret\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def folder_list(path,label):\n",
    "    '''\n",
    "    Take path to a directory, apply the label the contained documents,\n",
    "    and return the list of documents\n",
    "    PARAMETER PATH IS THE PATH OF YOUR LOCAL FOLDER\n",
    "    Label is positive or negative\n",
    "    '''\n",
    "    filelist = os.listdir(path)\n",
    "    review = []\n",
    "    for infile in filelist:\n",
    "        myfile = os.path.join(path,infile)\n",
    "        r = read_data(myfile)\n",
    "        r.append(label)\n",
    "        r.append(infile) #add file id for later reference\n",
    "        review.append(r)\n",
    "    return review\n",
    "\n",
    "def read_data(file_path):\n",
    "    '''\n",
    "    Read each file into a list of strings. \n",
    "    Example:\n",
    "    [\"it's\", 'a', 'curious', 'thing', \"i've\", 'found', 'that', 'when', 'willis', 'is', 'not', 'called', 'on', \n",
    "    ...'to', 'carry', 'the', 'whole', 'movie', \"he's\", 'much', 'better', 'and', 'so', 'is', 'the', 'movie']\n",
    "    '''\n",
    "    f = open(file_path)\n",
    "    lines = f.read().split(' ')\n",
    "    symbols = '${}()[].,:;+-*/&|<>=~\"_`\\' '\n",
    "    words = map(lambda Element: Element.translate(None, symbols).strip(), lines)\n",
    "    words = filter(None, words)\n",
    "    return words\n",
    "    \n",
    "def train_test_split(data,pivot_index):\n",
    "    '''\n",
    "    split list of documents into training and test sets\n",
    "    '''\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    train = data[:pivot_index]\n",
    "    test = data[pivot_index:]\n",
    "    assert len(train)==pivot_index, \"Training set is not proper length\"\n",
    "    return train,test\n",
    "\n",
    "def load_data():\n",
    "    '''\n",
    "    pos_path is where you save positive review data.\n",
    "    neg_path is where you save negative review data.\n",
    "    '''\n",
    "    home_dir = os.getcwd()\n",
    "    pos_path = os.path.join(home_dir,\"data/pos\")\n",
    "    neg_path = os.path.join(home_dir,\"data/neg\")\n",
    "\n",
    "    pos_review = folder_list(pos_path,1)\n",
    "    neg_review = folder_list(neg_path,-1)\n",
    "\n",
    "    review = pos_review + neg_review\n",
    "    return train_test_split(review,1500)\n",
    "\n",
    "train,test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sparse Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. Write a function that converts an example (e.g. a list of words) into a sparse bag-of-words\n",
    "representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lst2bag(lst):\n",
    "    bags = map(lambda x:collections.Counter(x[:-2]),lst)\n",
    "    labels = map(lambda x:x[-2],lst)\n",
    "    ids = map(lambda x:x[-1],lst)\n",
    "    return bags, labels, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Support Vector Machine via Pegasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1. Compute a subgradient for the “stochastic” SVM objective, which assumes a single training point. Show that if your step size rule is $\\eta_t=\\frac{1}{\\lambda_t}$, then the corresponding SGD update is the same as given in the pseudocode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> The stochastic SVM objective function is $$J(w)=\\min\\limits_{w}\\frac{\\lambda}{2}\\|w\\|^2+\\frac{1}{m}\\sum_{i=1}^{m}(1-y_iw^Tx_i)_+$$\n",
    "\n",
    "> $$J_i(w)=\\begin{cases}\n",
    "\\frac{\\lambda}{2m}\\|w\\|^2+1-y_iw^Tx_i & \\text{if $y_iw^Tx_i<1$,}\\\\\n",
    "\\frac{\\lambda}{2m}\\|w\\|^2 & \\text{otherwise.}\n",
    "\\end{cases}$$\n",
    "\n",
    "> $$\\nabla_w J_i(w)=\\frac{\\lambda w}{m}-\\mathbb 1(y_iw^Tx_i<1)y_ix_i$$\n",
    "\n",
    "> $$w_t = w_{t-1}-\\eta_t\\nabla_w J_i(w_{t-1})=(1-\\eta_tw_{t-1})w_{t-1}-\\mathbb 1(y_iw^Tx_i<1)\\eta_iy_ix_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2. Implement the Pegasos algorithm to run on a sparse data representation. The output should be a sparse weight vector w. Note that our Pegasos algorithm starts at w = 0. In a sparse representation, this corresponds to an empty dictionary. With this problem, you will need to take some care to code things efficiently. In particular, be aware that making copies of the weight dictionary can slow down your code significantly. If you want to make a copy of your\n",
    "weights (e.g. for checking for convergence), make sure you don’t do this more than once per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dotProduct(d1, d2):\n",
    "    \"\"\"\n",
    "    @param dict d1: a feature vector represented by a mapping from a feature (string) to a weight (float).\n",
    "    @param dict d2: same as d1\n",
    "    @return float: the dot product between d1 and d2\n",
    "    \"\"\"\n",
    "    if len(d1) < len(d2):\n",
    "        return dotProduct(d2, d1)\n",
    "    else:\n",
    "        return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
    "\n",
    "def increment(d1, scale, d2):\n",
    "    \"\"\"\n",
    "    Implements d1 += scale * d2 for sparse vectors.\n",
    "    @param dict d1: the feature vector which is mutated.\n",
    "    @param float scale\n",
    "    @param dict d2: a feature vector.\n",
    "\n",
    "    NOTE: This function does not return anything, but rather\n",
    "    increments d1 in place. We do this because it is much faster to\n",
    "    change elements of d1 in place than to build a new dictionary and\n",
    "    return it.\n",
    "    \"\"\"\n",
    "    for f, v in d2.items():\n",
    "        if f in d1:\n",
    "            d1[f] = d1.get(f, 0) + v * scale\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timing\n",
    "\n",
    "def pegasos(X,y,W_init,Lambda,max_epoch):\n",
    "    m = len(X)\n",
    "    t = 0\n",
    "    epoch=0\n",
    "    W = w_init.copy()\n",
    "    while epoch<max_epoch:\n",
    "        for j in range(m):\n",
    "            t+=1\n",
    "            step = 1/(t*Lambda)\n",
    "            for feature,value in W.items():\n",
    "                W[feature] = value*(1-step*Lambda)\n",
    "            if( y[j]*dotProduct(W,X[j]) )<1:\n",
    "                increment(W,step*y[j],X[j])\n",
    "        epoch+=1\n",
    "        print(epoch)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initW(bags):\n",
    "    #     Get full dictionary\n",
    "    c = collections.Counter()\n",
    "    for review in bags:\n",
    "        c.update(review)\n",
    "    w = {}\n",
    "    for key,value in c.items():\n",
    "        w[key] = 0\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag_train,label_train,ids_train = lst2bag(train)\n",
    "bag_test,label_test,ids_test = lst2bag(test)\n",
    "w_init = initW(bag_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "pegasos function took 91160.556 ms\n"
     ]
    }
   ],
   "source": [
    "w = pegasos(bag_train,label_train,w_init,0.1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3. Implement the Pegasos algorithm with change in update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timing\n",
    "def implemented_pegasos(X,y,W_init,Lambda,max_epoch):\n",
    "    m = len(X)\n",
    "    t=s=1\n",
    "    epoch=0\n",
    "    W = W_init.copy()\n",
    "    while epoch<max_epoch:\n",
    "        for j in range(m):\n",
    "            t+=1\n",
    "            step = 1/(t*Lambda)\n",
    "            s=(1-step*Lambda)*s\n",
    "            if( y[j]*dotProduct(W,X[j]) )<1:\n",
    "                increment(W,step*y[j]/s,X[j])\n",
    "        epoch+=1\n",
    "    #Scale w:\n",
    "    w={}\n",
    "    for f,v in W.items():\n",
    "        w[f]=v*s \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implemented_pegasos function took 11591.152 ms\n"
     ]
    }
   ],
   "source": [
    "w_imp = implemented_pegasos(bag_train,label_train,w_init,1,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4. Run both implementations of Pegasos to train an SVM on the training data (using the bag-of-words feature representation described above). Make sure your implementations are correct by verifying that the two approaches give essentially the same result. Report on the time taken to run each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> The naive pegasos has the same parameters as the implemented pegasos. The implemented Pegasos is running 206.4ms per epoch, while the previous pegasos is running 86861ms per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.5. Write a function that takes a sparse weight vector w and a collection of $(x,y)$ pairs, and returns the percent error when predicting y using $sign(w^Tx)$. In other words, the function reports the 0-1 loss of the linear predictor $x\\mapsto w^Tx.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_err(X,y,w):\n",
    "    num = 0\n",
    "    for i in range(len(X)):\n",
    "        if dotProduct(w,X[i])*y[i]<0:\n",
    "            num += 1\n",
    "    return num/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.146"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_err(bag_test,label_test,w_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.6. Using the bag-of-words feature representation described above, search for the regularization\n",
    "parameter that gives the minimal percent error on your test set. A good search strategy is to\n",
    "start with a set of lambdas spanning a broad range of orders of magnitude. Then, continue to\n",
    "zoom in until you’re convinced that additional search will not significantly improve your test\n",
    "performance. Once you have a sense of the general range of regularization parameters that\n",
    "give good results, you do not have to search over orders of magnitude every time you change\n",
    "something (such as adding new feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lambda_search(lambdas,X,y):\n",
    "    loss_history = []\n",
    "    f = True\n",
    "    loss_opt=0\n",
    "    for Lambda in lambdas:\n",
    "        w_search = implemented_pegasos(X,y,w_init,Lambda,50)\n",
    "        loss = get_err(bag_test,label_test,w_search)\n",
    "        loss_history.append(loss)\n",
    "        print(\"Lambda is {0}, err is {1}.\".format(Lambda,loss))\n",
    "        if f or loss<loss_opt:\n",
    "            f=False\n",
    "            loss_opt = loss\n",
    "            w_opt = w_search\n",
    "            lambda_opt = Lambda        \n",
    "    return lambda_opt,w_opt,loss_opt,loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implemented_pegasos function took 15003.746 ms\n",
      "Lambda is 100.0, err is 0.146.\n",
      "implemented_pegasos function took 14886.406 ms\n",
      "Lambda is 21.5443469003, err is 0.144.\n",
      "implemented_pegasos function took 14964.526 ms\n",
      "Lambda is 4.64158883361, err is 0.148.\n",
      "implemented_pegasos function took 14910.793 ms\n",
      "Lambda is 1.0, err is 0.146.\n",
      "implemented_pegasos function took 15189.747 ms\n",
      "Lambda is 0.215443469003, err is 0.152.\n",
      "implemented_pegasos function took 14853.362 ms\n",
      "Lambda is 0.0464158883361, err is 0.152.\n",
      "implemented_pegasos function took 14869.276 ms\n",
      "Lambda is 0.01, err is 0.152.\n",
      "implemented_pegasos function took 14781.000 ms\n",
      "Lambda is 0.00215443469003, err is 0.152.\n",
      "implemented_pegasos function took 14835.016 ms\n",
      "Lambda is 0.000464158883361, err is 0.152.\n",
      "implemented_pegasos function took 14777.935 ms\n",
      "Lambda is 0.0001, err is 0.152.\n",
      "Best lambda = 21.5443469003\n"
     ]
    }
   ],
   "source": [
    "Lambdas=map(lambda x:10**x,np.linspace(2,-4,10))\n",
    "lambda_opt,w_opt, loss_opt, loss_history = lambda_search(Lambdas,bag_train,label_train)\n",
    "print('Best lambda = {0}'.format(lambda_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEdCAYAAAAvj0GNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncrCYmVlSDhYzM0vq/wO/45WRLGSTKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b0d4490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Lambdas,loss_history)\n",
    "plt.ylim(0.14,0.16)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Pct. Error')\n",
    "plt.xscale('log')\n",
    "plt.title('Pct. Error vs. Lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implemented_pegasos function took 15045.886 ms\n",
      "Lambda is 1.0, err is 0.146.\n",
      "implemented_pegasos function took 14787.031 ms\n",
      "Lambda is 0.889, err is 0.152.\n",
      "implemented_pegasos function took 15012.624 ms\n",
      "Lambda is 0.778, err is 0.152.\n",
      "implemented_pegasos function took 14848.043 ms\n",
      "Lambda is 0.667, err is 0.152.\n",
      "implemented_pegasos function took 14910.439 ms\n",
      "Lambda is 0.556, err is 0.152.\n",
      "implemented_pegasos function took 14968.291 ms\n",
      "Lambda is 0.445, err is 0.152.\n",
      "implemented_pegasos function took 554500.128 ms\n",
      "Lambda is 0.334, err is 0.152.\n",
      "implemented_pegasos function took 16446.209 ms\n",
      "Lambda is 0.223, err is 0.152.\n",
      "implemented_pegasos function took 16492.498 ms\n",
      "Lambda is 0.112, err is 0.152.\n",
      "implemented_pegasos function took 15603.160 ms\n",
      "Lambda is 0.001, err is 0.152.\n",
      "Best lambda = 1.0\n"
     ]
    }
   ],
   "source": [
    "Lambdas=np.linspace(1,0.001,10)\n",
    "lambda_opt,w_opt, loss_opt, loss_history = lambda_search(Lambdas,bag_train,label_train)\n",
    "print('Best lambda = {0}'.format(lambda_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEdCAYAAAAvj0GNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSWrKYJEkNWWwSJKa+v/0rONvaf1GjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ca4c910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Lambdas,loss_history)\n",
    "plt.ylim(0.14,0.16)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Pct. Error')\n",
    "plt.xscale('log')\n",
    "plt.title('Pct. Error vs. Lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.7. [Optional] Recall that the “score” is the value of the prediction $f(x)=w^Tx$. We like to think that the magnitude of the score represents the confidence of the prediction. This is something we can directly verify or refute. Break the predictions into groups based on the score (you can play with the size of the groups to get a result you think is informative). For each group, examine the percentage error. You can make a table or graph. Summarize the results. Is there a correlation between higher magnitude scores and accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.146"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_err(bag_test,label_test,w_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>err</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(-0.13, -0.0942]</th>\n",
       "      <td>-0.639351</td>\n",
       "      <td>-6</td>\n",
       "      <td>-6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-0.0942, -0.0583]</th>\n",
       "      <td>-1.200837</td>\n",
       "      <td>-17</td>\n",
       "      <td>-17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-0.0583, -0.0225]</th>\n",
       "      <td>-3.739737</td>\n",
       "      <td>-86</td>\n",
       "      <td>-98</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(-0.0225, 0.0133]</th>\n",
       "      <td>-0.944961</td>\n",
       "      <td>-52</td>\n",
       "      <td>-58</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0133, 0.0491]</th>\n",
       "      <td>3.668644</td>\n",
       "      <td>97</td>\n",
       "      <td>129</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0491, 0.0849]</th>\n",
       "      <td>2.900281</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.0849, 0.121]</th>\n",
       "      <td>1.012267</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.121, 0.157]</th>\n",
       "      <td>0.141251</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.157, 0.192]</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(0.192, 0.228]</th>\n",
       "      <td>0.228237</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       score  label  prediction  err\n",
       "score                                               \n",
       "(-0.13, -0.0942]   -0.639351     -6          -6    0\n",
       "(-0.0942, -0.0583] -1.200837    -17         -17    0\n",
       "(-0.0583, -0.0225] -3.739737    -86         -98    6\n",
       "(-0.0225, 0.0133]  -0.944961    -52         -58   51\n",
       "(0.0133, 0.0491]    3.668644     97         129   16\n",
       "(0.0491, 0.0849]    2.900281     46          46    0\n",
       "(0.0849, 0.121]     1.012267     10          10    0\n",
       "(0.121, 0.157]      0.141251      1           1    0\n",
       "(0.157, 0.192]           NaN    NaN         NaN  NaN\n",
       "(0.192, 0.228]      0.228237      1           1    0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "for bag in bag_test:\n",
    "    score = dotProduct(w_opt, bag)\n",
    "    scores.append(score)\n",
    "\n",
    "score_df=pd.DataFrame(zip(scores,label_test))\n",
    "score_df.columns=('score','label')\n",
    "score_df['prediction']=np.sign(score_df['score'])\n",
    "score_df['err']= map(lambda x:int(x),score_df['label']!=score_df['prediction'])\n",
    "groups = score_df.groupby(pd.cut(score_df.score,10))\n",
    "groups.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> Scores $\\in [-0.0583,0.0491]$ has most percentage error. So the larger the magitude of score, the more accurate the prediction is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.8. [Optional] Our objective is not differentiable when $w^Tx_i=0$. Investigate how often and when we have $w^Tx_i=0$. Describe your findings. If we didn’t know about subgradients, one might suggest just skipping the update when $w^Tx_i=0$. Does this seem reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Error Analysis\n",
    "Choose some examples that the model got wrong. List the features that contributed most heavily to the descision (e.g. rank them by $\\|w_ix_i\\|$), along with $x_i$, $w_i$, $xw_i$. Do you understand why the model was incorrect? Can you think of a new feature that might be able to fix the issue? Include a short analysis for at least 2 incorrect examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> The first misclassified test file is cv580_14064.txt. It's a really strange review, the author holds an almost neutral attitude and even don't recommend at the last. The word \"this\" and \"movie\" classified as negative, which will help if we remove them.\n",
    "\n",
    "> The second file contains many \"and\" and \"the\". We may want to remove these stopwords to improve it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def example_analysis(i):\n",
    "    print(\"File\",ids_test[i])\n",
    "    print(\"True sentiment: \",label_test[i], \n",
    "          \".  Predicted sentiment: \",wrong.loc[i,'prediction'], \n",
    "          \". Score: \", wrong.loc[i,\"score\"])\n",
    "    f = open(wrong.loc[i,'name'])\n",
    "    lines = f.read()\n",
    "    print(lines)\n",
    "    bow = bag_test[i]\n",
    "    feature_dict = {}\n",
    "    for key in bow:\n",
    "        w=w_opt[key] if (key in w_opt.keys()) else np.nan\n",
    "        x=bow[key]\n",
    "        feature_dict[key]={'abs_xw':abs(x*w),'x':x,'w':w,'xw':x*w}\n",
    "    feature_df = pd.DataFrame.from_dict(feature_dict,'index')\n",
    "    print(feature_df.sort('abs_xw',ascending=False)[:10],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:16: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cv580_14064.txt\n",
      "True sentiment:  1 .  Predicted sentiment:  -1.0 . Score:  -0.0267329768936\n",
      "this movie was one of the first american films done by director paul verhoeven . \n",
      "he has done films since then such as basic instict and starship troopers . \n",
      "all of his movies have one main thing in common- they don't skimp on the violence . \n",
      "in a verhoeven movie a guy doesn't get shot once in the chest- he gets shot many times all over the body . \n",
      "robocop takes place in detroit in the future . \n",
      "things are pretty bleak . \n",
      "the company ocp rules the city with an iron fist and is moving in on running the police force . \n",
      "peter weller plays a cop who goes on a mission with his partner , nancy allen . \n",
      "peter weller goes in an empty room alone and gets killed . \n",
      "meanwhile ocp has a failed demonstration of a ed-209 model robot crime fighter . \n",
      "someone comes up with the idea of having a cyborg robocop so they use any parts worth scalvaging from peter weller and make him into the robocop the movie is titled after . \n",
      "the plot of this movie generally runs at a decent pace . \n",
      "it drags a bit towards the end with robocop wanting to finish everybody off , but plot twists keep it exciting . \n",
      "the acting is bad from weller when he is human , but very good when he is robocop . \n",
      "nancy allen does a decent job . \n",
      "this movie doesn't leave itself open for a sequel exactly . \n",
      "but they made one anyway . \n",
      "enter . . . \n",
      "robocop 2 starring : peter weller and nancy allen directed by : irvin kershner \n",
      "detroit is in a more bleak state than it was in the first film . \n",
      "ocp is trying to develop an improved 'robocop 2' that will be more than a match for criminals . \n",
      "most of the police force is on strike due to ocp taking it over , leaving robocop to be the police force in the city . \n",
      "this film is pretty bizaare . \n",
      "it has a 13 year-old drug-lord who curses as much as eddie murphy , even more violence than the original , and some more humor . \n",
      "the plot of this movie is pretty large , almost too much so . \n",
      "there are three or four plot lines weaving around that don't really match up . \n",
      "the effects are still decent though . \n",
      "this sequel isn't more of the same really , just a darker version robocop dealing more with drugs than with armed criminals . \n",
      "if you liked the first one , watch this . \n",
      "but be dissapointed . \n",
      "so later comes . . . \n",
      "robocop 3 starring nancy allen and robert john burke \n",
      "ocp is now owned by a large japanese company . \n",
      "robocop joins a band of rebels to counter-attack the evil empire , i mean ocp . \n",
      "the japanese send a cyborg-ninja to deal with robocop . \n",
      "among other things . . . \n",
      "this movie is easier to follow than the last one . \n",
      "which means the plot isn't quite as crowded . \n",
      "this movie isn't anything special though . \n",
      "just more of robocop blasting things up in a more kid-friendly way . \n",
      "really not reccomended . \n",
      "\n",
      "         x         w    abs_xw        xw\n",
      "plot     5 -0.002187  0.010933 -0.010933\n",
      "this     9 -0.000987  0.008880 -0.008880\n",
      "and      7  0.001253  0.008773  0.008773\n",
      "the     27  0.000307  0.008280  0.008280\n",
      "he       4  0.001413  0.005653  0.005653\n",
      "on       4 -0.001200  0.004800 -0.004800\n",
      "bad      1 -0.004760  0.004760 -0.004760\n",
      "movie    8 -0.000453  0.003627 -0.003627\n",
      "though   2  0.001800  0.003600  0.003600\n",
      "is      12  0.000293  0.003520  0.003520 \n",
      "\n",
      "File cv041_22364.txt\n",
      "True sentiment:  -1 .  Predicted sentiment:  1.0 . Score:  0.00106665244463\n",
      "out of sight director steven sorderbergh baffles the hell out of us all in the limey , a cold , uninvolving , confusing new thriller . \n",
      "though the plot description may at first seem like it came from the pen of elmore leonard ( author of out of sight , as well as jackie brown , get shorty and pulp fiction ) , after you watch it , you realize that it's not nearly good enough . \n",
      "in an aggressively non-linear fashion , the limey ( li * mey , noun : an english gentleman ) tells the story of wilson ( terrence stamp ) , a british ex-con just released from a 9 year stint in prison for armed robbery . \n",
      "he has come to the us to seek vengeance for the death of his daughter jenny . \n",
      "he doesn't know much about the circumstances of her demise , all he has is a name : terry valentine . \n",
      "valentine was jenny's former boyfriend , a wealthy and corrupt record executive . \n",
      "he's played by peter fonda , in his first major role since the terrific ulee's gold in 1997 . \n",
      "seeking valentine's reclusive place of residence turns out to be no easy task for wilson . \n",
      "he finally finds the impressive abode high in the mountains and sneaks in just as valentine is having a big party . \n",
      "he winds up breaking his cover eventually , setting off valentine's head of security and valentine himself , who decides to run for it . \n",
      "what a mess . \n",
      "i have no problem when films refuse to be constricted by the linearity of time -- pulp fiction , which twisted time every which way , was a masterpiece -- but i do take exception to movies that decide to play around with it for no reason other than to confuse the viewer . \n",
      "the limey does exactly that . \n",
      "the plot is permeated with flashbacks , flash-forwards and what can only be described as random time-travel , without any evident purpose . \n",
      "there is no method to this movie's madness . \n",
      "it uses a fancy way to tell a story that would be better off told more conventionally and more comprehendably . \n",
      "the plot isn't particularly interesting in the first place : traditional , mildly hackneyed and not very involving . \n",
      "this is a sort of brooding film -- our protagonist doesn't speak much and the action sequences are done with an annoyingly perfunctory attitude . \n",
      "i felt like the director wasn't very interested in the proceedings himself , almost like he made this film for a paycheck . \n",
      "ditto for the editing , which seems to be deliberately sloppy and unpleasant . \n",
      "sixties icon terrence stamp manages to at least be menacing as the aging criminal . \n",
      "he's not much in the way of stature but he has a surprisingly imposing physical presence that works to his advantage here . \n",
      "peter fonda is an unbelievably underrated actor : he's shy , quiet but always effective . \n",
      "he's adept at conveying emotions through speech rather than expression : his feelings don't always show on his face by you can always tell what they are . \n",
      "this is basically a conventional thriller told in a pretentiously bizarre fashion . \n",
      "why soderbergh couldn't just parrot down and tell a story , i don't know , but what he does do certainly doesn't work . \n",
      "the result is a wild cornucopia of images that amount to precisely nil -- even the action scenes don't work . \n",
      "1999 may have signified the death of the traditional act one - act two - act three storyline , but obviously some movies have not yet transcended it . \n",
      "shall we go back to basics ? \n",
      "\n",
      "       x         w    abs_xw        xw\n",
      "and   10  0.001253  0.012533  0.012533\n",
      "he     8  0.001413  0.011307  0.011307\n",
      "the   28  0.000307  0.008587  0.008587\n",
      "no     4 -0.001787  0.007147 -0.007147\n",
      "plot   3 -0.002187  0.006560 -0.006560\n",
      "to    15 -0.000413  0.006200 -0.006200\n",
      "way    3  0.001773  0.005320  0.005320\n",
      "what   4  0.001213  0.004853  0.004853\n",
      "have   3 -0.001533  0.004600 -0.004600\n",
      "you    3  0.001467  0.004400  0.004400 \n",
      "\n",
      "File cv605_12730.txt\n",
      "True sentiment:  -1 .  Predicted sentiment:  1.0 . Score:  0.0272263036493\n",
      "a fullyloaded entertainment review : website coming soon ! \n",
      "hunter s . thompson's fear and loathing in las vegas , written in 1971 , is already an american classic ; not merely because it was an unadulterated journey through the post-psychedelia of the 1960's , but also because it ushered in a new form of journalism known as \" gonzo \" . \n",
      "soon after the book was written , hunter s . thomspon became the basis for a character in doonesbury known as uncle duke . \n",
      "and that is the problem with fear and loathing in las vegas ( the movie . ) \n",
      "although the book is one of my favorites , i now agree with thompson's own assessment of the novel as \" unfilmable . \" \n",
      "the main problem is that the book , although detailing wild drug use and tripped-out adventures , was not written on the road . \n",
      "it was written and edited by hunter s . thomspons , relatively sober , at his home . \n",
      "so although the book depicts drug use and still more drug use , it is told with a sense of aloofness and some humor . \n",
      "hunter s . thomspon , the narrator , while always drugged out , still has a grip on reality , and can still tell , with journalistic ability , what is going on around him . \n",
      "not so with the characters in fear and loathing in las vegas . \n",
      "despite name changes and one minor scene from the book dropped , the movie is an exact duplicate of the book , in terms of dialouge . \n",
      "unfortunately , this can get annoying , in the form of johnny depp's ( who plays thompson ) excessive narration . \n",
      "and the characters , while having vivid personalities , are played as 2-d characers on the screen : dr . raoul duke ( johnny depp , the name raoul duke was an alias of thompson's ) and his riding companion , dr . gonzo ( benicio del toro ) , who is a large samoan lawyer , have no personalities or emotions at all , save for stoned . \n",
      "the film is very faithful to the plot ( what there is ) of the book , in that dr . raoul duke is sent by sports illustrated to cover the mint 400 , and takes along not only his samoan lawyer but also car trunk full of drugs . \n",
      "from there , one set of stoned adventures after another happens , including one of the few good scenes in which depp , high on acid , watches all the people in the bar around him turn into , literally , lounge lizards . \n",
      "while the movie has its moments ( few ) , it is almost unwatchable at parts , especially towards the end . \n",
      "despite a lot of celebrity cameos , the stars never get off the ground . \n",
      "the set decoration and costumes are great , and probably deserve an oscar nomination . \n",
      "however , as we all know , it's not sets or effects that make a movie , its the characters ; and quite frankly , these characters are no good . \n",
      "\n",
      "        x         w    abs_xw        xw\n",
      "and    16  0.001253  0.020053  0.020053\n",
      "the    34  0.000307  0.010427  0.010427\n",
      "s       4 -0.001547  0.006187 -0.006187\n",
      "on      5 -0.001200  0.006000 -0.006000\n",
      "also    2  0.002173  0.004347  0.004347\n",
      "book    7  0.000613  0.004293  0.004293\n",
      "an      5 -0.000840  0.004200 -0.004200\n",
      "no      2 -0.001787  0.003573 -0.003573\n",
      "is     12  0.000293  0.003520  0.003520\n",
      "while   3  0.001173  0.003520  0.003520 \n",
      "\n",
      "File cv893_26269.txt\n",
      "True sentiment:  1 .  Predicted sentiment:  -1.0 . Score:  -0.00251996640045\n",
      "let me open this one with a confession : i love cop movies . \n",
      "i adore them with such an unwavering , near foolish passion that i was actually one of the first in line to see 1998's one tough cop starring the smirkiest of the baldwin brothers , steven . \n",
      "was it any good ? \n",
      "well , just about as good as baldwin's steven seagal impression ( for those i've confused the answer is a resounding no ! ) . \n",
      "now let me clarify , i don't love just any cop movie ; i'm not too keen on the smug buddy cop flicks that were so in vogue in the 80's and now slowly ( gulp ) returning thanks to jackie chan and chris tucker . \n",
      "i'm partial to those gritty , earthy cop dramas like serpico , donnie brasco , and the french connection . \n",
      "problem is these films come along just about as often as george w . bush makes a cogent point so whenever a \" true \" cop movie does hit multiplexes , i come running towards it like an eager puppy ? ? ? and inevitably i leave , head bowed , tripping over my squiggly tail . \n",
      "so what is it about cop movies that transform me into a moron willing to shell out $7 . 50 for a movie called one tough cop ( which actually sounds a lot like a wiseguy spin-off , no ? ) ? \n",
      "truth is i'm a sucker for those barren streets , that gritty , graffiti strewn modernist jungle where dangerous criminals lurk and cops , like modern day knights , must shield us from their iniquitous clutches . \n",
      "yes it's all very corny and awfully boys clubish , which , i'm guessing , is precisely where these feelings come from . \n",
      "as little tots us boys are brought up to create our own wars with gi joe's , enact cops and robbers or cowboy and indian fantasies , etc . the best gritty cop films tap into those gleeful adolescent fantasies , and heighten them , turning it all into an existential nightmare of thin moral lines and psychological chaos . \n",
      "as an adult my tastes haven't changed all that drastically ; sure i can get a cerebral high from the emotional complexities of a krzysztof kieslowski film , but i still delight at the promise a \" guy \" film could potentially bring . \n",
      "now what if it's the cops who're the corrupt ones ? \n",
      "ooh that one really gets me going , these protectors using all that at their service , taking advantage of others for their own gain . \n",
      "throw a courtroom drama ( yech ! ) \n",
      "into the pot and you have the tremendously absorbing sidney lumet film prince of the city . \n",
      "how absorbing is prince of the city , you ask ? \n",
      "so damn absorbing that when the film shifts from gritty undercover cop drama to courtroom drama ( a sub-genre i loath the way kristy swanson must loath sarah michelle gellar ) i remained completely and utterly involved . \n",
      "the film stars treat williams , who gives the kind of performance that should have skyrocketed his career up to the level that brad pitt currently resides at . \n",
      "since i was hatched a mere two years prior to the release of prince of the city , i cannot provide an answer as to why it didn't . \n",
      " ( though judging from his recent roles in tripe like deep rising and dead heat i would guess that it probably has something to do with quite a few bad career moves ) . \n",
      "like mickey rourke and eric roberts ( both of whom are currently doing time in direct to video purgatory ) williams is a talented leading man who didn't make it to where he should have . \n",
      " ( if there was any justice in this dog gone world keanu reeves would be the one banished to direct to video hell ) . \n",
      "here , the actor is given a role not all that dissimilar from al pacino's in serpico ( also directed by lumet ) . \n",
      "williams plays danny ciello , a member of an elite squad of police officers who take down drug dealers on the gritty pre-juliani streets of new york . \n",
      "they work without uniforms or hours much like paid vigilantes . \n",
      "and that analogy is more than apt seeing as how the bunch are kinda sorta forced into using corrupt means to get the job done . \n",
      "the squad relies entirely on informants ( low level junkies ) to gain access to the big wigs , and in order to obtain the info they seek , they sometimes have to provide the junkies with the junk of their choice . \n",
      "danny has done this , and though he's a fairly honorable guy , who took the job in order to do good , his intentions have gotten a bit skewed along the way . \n",
      "he seeks redemption and concludes that the best way to go about getting it is by talking to internal affairs who then convince him to go undercover as a \" double agent \" . \n",
      "he does so with one condition : he will not rat on his partners . \n",
      "right away we know that that is exactly what he'll have to do . \n",
      "and watching the movie build to that is thrilling in the way that any great character driven story can be . \n",
      "undercover cop films are almost innately exciting , even the kind that pull out all the cliches ( like in too deep ) . \n",
      "how can one not be moved by the issues of betrayal , ambiguity , and the danger in putting oneself in such a risky position ? \n",
      "prince of the city takes all this and charges it with a perceptive , hard boiled script and taut unintrusive direction by old pro sidney lumet , as well as a handful of intense performances , most notably by treat . \n",
      "lumet's greatest contribution to the movie is how he lays back , and lets the camera soak up the action . \n",
      "his scenes transition rapidly from one to the next building layers of tension , and he rarely holds for reaction shots , instead he clicks away at precisely the moment a character stops yakking . \n",
      "lumet's worst tendency is one that many film makers ( especially oliver stone ) tend to overuse : he relies a little too heavily on actors shouting their lines at each other presumably in order to ratchet up the tension . \n",
      "this isn't as much a problem here as it was in night falls on manhattan , lumet's most recent cop \\ lawyer endeavor , a movie that nearly gave me an ulcer from just watching . \n",
      "though of late lumet has made several really awful career choices ( maybe he and treat should get together for a drink or something ) , the most recent a completely unnecessary remake of gloria , here he was on , sustaining tension in a film that runs for 167 minutes with not one dull spot . \n",
      "some of the formula pictures i like the best are the most simplistic . \n",
      "hollywood often forgets how easy it is to make these kinds of pictures even tolerable . \n",
      "of course cop movies aren't popular anymore ( yes buddy cop flicks are popular but those are more like extended sitcoms with gunfire amid the punch lines ) , but just look at the average hollywood action movies , flicks so bloated and full of unnecessary fat like the boring love interest that serves no purpose other than to stall the action and demonstrate that our hero isn't gay , the lame comic relief character who's never funny , and on and on . \n",
      "for all its moral ambiguities prince of the city is a back to the basics cop \\ courtroom drama that leaves out all the crap and really thrills . \n",
      "it focuses completely on its story , and its characters all of whom are completely believable from the cops to the lawyers to the junkies . \n",
      "there's a great scene early on where an irate williams shouts at an ia officer and looks as if he's about to go into one of those corny braveheart inspirational speeches , but instead he breaks down in front of the cop and begins sobbing . \n",
      "this is a scene that would never make it into a michael bay film ( even though bruce willis sacrifices his life at the end of armageddon , not one damn tear ) it would be cut out because the director would say oh that makes him too human , excuse me , weak . \n",
      "in the very next scene danny speaks in a hushed raspiness , straining to get the words out . \n",
      "he's lost his voice . \n",
      "does that little itty bitty detail matter ? \n",
      "maybe not to most , but it sure made my night . \n",
      "and in a sprawling epic of corruption , greed , betrayal and violence it's nice to see that someone is paying attention to the little things . \n",
      "\n",
      "       x         w    abs_xw        xw\n",
      "and   35  0.001253  0.043866  0.043866\n",
      "the   70  0.000307  0.021466  0.021466\n",
      "to    39 -0.000413  0.016120 -0.016120\n",
      "he    11  0.001413  0.015546  0.015546\n",
      "on    11 -0.001200  0.013200 -0.013200\n",
      "all   10 -0.001040  0.010400 -0.010400\n",
      "most   5  0.001960  0.009800  0.009800\n",
      "have   6 -0.001533  0.009200 -0.009200\n",
      "at     8 -0.001133  0.009067 -0.009067\n",
      "like  11 -0.000800  0.008800 -0.008800 \n",
      "\n",
      "File cv828_19831.txt\n",
      "True sentiment:  1 .  Predicted sentiment:  -1.0 . Score:  -0.0109331875575\n",
      "all those who were offended by there's something about mary should not tread in the waters of american pie , a gratuitously sexual rollercoaster ride of raunch . \n",
      "and while this uproariously funny , gross-out summer movie is basically an exercise in bad taste , it also demonstrates a surprising sweetness in the end . \n",
      "american pie is the latest entry to the offensive teenage-targeted fare this season . \n",
      "i must question the studio that releases a movie like this . \n",
      "the producers are aiming these films at the 15-24 crowd , but while pushing the limit of possible sex and violence in an r-rated film . \n",
      "some have even been narrowly avoiding the dreaded nc-17 , a rating the mpaa gives when the adult content surpasses even that of a restricted movie . \n",
      "american pie was threatened with an nc-17 , but after snipping a few scenes from the finished product , it was given an r . but at least this new addition is not as careless and unforgiving as something like south park . \n",
      "it is sick , perverse , and ultimately disgusting - not to mention extremely funny . \n",
      "american pie is about four desperate teenagers who make a pact to lose their virginity by prom night . \n",
      "jim ( jason biggs ) , kevin ( thomas ian nicholas ) , oz ( chris klein ) and finch ( eddie kaye thomas ) are a quartet of high school students in their senior year who think sex is something they must experience to be successful in their lives ahead . \n",
      "jim inquires what it feels like when you reach third base , to which one of his friends replies : `like warm apple pie . . . ' . \n",
      "of course , this prompts the scene unfairly exposed in the trailers , in which jim and a freshly baked pie have a very intimate moment in the kitchen corner . \n",
      "there are many moments , such as this , in which director paul weitz uses a game-plan similar to the one frequently displayed in mary : to cause the audience to break down laughing in disgusting disbelief . \n",
      "american pie is a hard-fought effort that has replenishing rewards if you manage to stick with it . \n",
      "i loved the young cast in this movie . \n",
      "consider the subject matter that these actors have dealt with , obviously suggestiveness never rivaled in any of their previous projects , and you should appreciate their performances . \n",
      "biggs is more than enjoyable , and chris klein ( who recently played a similar jock in election ) is obviously a young talent on the rise . \n",
      "but in a teen-dominated movie , the best performance comes from comedian eugene levy ( of television's sctv ) , who is unexpectedly brilliant as jim's uneasy father . \n",
      "scenes in which the familiar father-son conversations are brought to interesting new levels are the funniest moments to be found in american pie . \n",
      "after levy discovers jim's new use for apple pie , he tells him , `i did a fare share of that sort of thing when i was your age . \n",
      "but i never used baked goods . ' \n",
      "it is performances such as his , and bill murray's in rushmore , that are often unfairly dismissed when awards are handed out . \n",
      "there are some less-than-original aspects of the film . \n",
      "a bathroom incident involving finch and a bottle of ex-lax is something we've seen before . \n",
      "if weitz is planning to take after the farrelly brothers , then this is a decidedly unwise move : the same prank was pulled in the brother's dumb and dumber , and used to greater effect . \n",
      "secondly , the characters are forced into a half-hearted climax that seems more artificial than amusing . \n",
      "but at one point , in which we discover that american pie does have a heart of it's own , biggs asks his buddies why he's going through so much pressure for something that's `not very important anyway . ' \n",
      "and so we realize that while the film is expressive about a subject that is hardly appropriate , it still manages to be mature . \n",
      "in it's own way . \n",
      "\n",
      "           x         w    abs_xw        xw\n",
      "and       13  0.001253  0.016293  0.016293\n",
      "the       29  0.000307  0.008893  0.008893\n",
      "american   7  0.001133  0.007933  0.007933\n",
      "this       8 -0.000987  0.007893 -0.007893\n",
      "have       4 -0.001533  0.006133 -0.006133\n",
      "to        14 -0.000413  0.005787 -0.005787\n",
      "bad        1 -0.004760  0.004760 -0.004760\n",
      "is        16  0.000293  0.004693  0.004693\n",
      "you        3  0.001467  0.004400  0.004400\n",
      "but        6 -0.000680  0.004080 -0.004080 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for bag in bag_test:\n",
    "    score = dotProduct(w_opt, bag)\n",
    "    scores.append(score)\n",
    "\n",
    "score_df=pd.DataFrame(zip(scores,label_test))\n",
    "score_df.columns=('score','label')\n",
    "score_df['prediction']=np.sign(score_df['score'])\n",
    "score_df['err']= map(lambda x:int(x),score_df['label']!=score_df['prediction'])\n",
    "wrong = score_df[score_df.err==1]\n",
    "wrong_name = []\n",
    "for w in wrong.iterrows():\n",
    "    if int(w[1].label)==1:\n",
    "        name = \"data/pos/\"\n",
    "    else:\n",
    "        name = \"data/neg/\"\n",
    "    name+=ids_test[w[0]]\n",
    "    wrong_name.append(name)\n",
    "wrong['name']=wrong_name\n",
    "for i in wrong.index[:5]:\n",
    "    example_analysis(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1. Based on your error analysis, or on some idea you have, construct a new feature (or group of features) that you hope will improve your test performance. Describe the features and what kind of improvement they give. At this point, it’s important to consider the standard errors $\\sqrt{p(1 − p)/n}$ on your performance estimates, to know whether the improvement is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(w):\n",
    "    '''\n",
    "    remove stopwords \n",
    "    Stopwords list from http://www.ranks.nl/stopwords\n",
    "    '''\n",
    "    f = open('stopwords.txt')\n",
    "    stopwords = f.read().split('\\n')\n",
    "    for k in stopwords:\n",
    "        w.pop(k, None)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@timing\n",
    "def implemented_pegasos(X,y,W_init,Lambda,max_epoch,remove_stops=False):\n",
    "    m = len(X)\n",
    "    t=s=1\n",
    "    epoch=0\n",
    "    W = W_init.copy()\n",
    "    print(len(W))\n",
    "    if remove_stops:\n",
    "        W = remove_stopwords(W)\n",
    "    while epoch<max_epoch:\n",
    "        for j in range(m):\n",
    "            t+=1\n",
    "            step = 1/(t*Lambda)\n",
    "            s=(1-step*Lambda)*s\n",
    "            if( y[j]*dotProduct(W,X[j]) )<1:\n",
    "                W = increment(W,step*y[j]/s,X[j])\n",
    "        epoch+=1\n",
    "    #Scale w:\n",
    "    w={}\n",
    "    for f,v in W.items():\n",
    "        w[f]=v*s \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initW(bags):\n",
    "    #     Get full dictionary\n",
    "    c = collections.Counter()\n",
    "    for review in bags:\n",
    "        c.update(review)\n",
    "    w = {}\n",
    "    for key,value in c.items():\n",
    "        w[key] = 0\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(X,y,remove_stops=False, Lambda=0.1):\n",
    "    w = initW(X)\n",
    "    w_opt = implemented_pegasos(X,y,w,1,50,remove_stops)\n",
    "    pct_err = get_err(bag_test,label_test,w_opt)\n",
    "    std_err = np.sqrt(pct_err*(1-pct_err)/len(bag_test))\n",
    "    print(\"Percent Error: %f +/- %f\" %(pct_err,std_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag_train,label_train,ids_train=lst2bag(train)\n",
    "bag_test,label_test,ids_test=lst2bag(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction without removing stopwords.\n",
      "41754\n",
      "implemented_pegasos function took 11244.202 ms\n",
      "Percent Error: 0.146000 +/- 0.015791\n"
     ]
    }
   ],
   "source": [
    "print('Prediction without removing stopwords.')\n",
    "evaluate_model(bag_train,label_train,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after removing stopwords.\n",
      "41754\n",
      "implemented_pegasos function took 10364.115 ms\n",
      "Percent Error: 0.174000 +/- 0.016954\n"
     ]
    }
   ],
   "source": [
    "print('Prediction after removing stopwords.')\n",
    "evaluate_model(bag_train,label_train,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:16: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cv580_14064.txt\n",
      "True sentiment:  1 .  Predicted sentiment:  -1.0 . Score:  -0.0267329768936\n",
      "this movie was one of the first american films done by director paul verhoeven . \n",
      "he has done films since then such as basic instict and starship troopers . \n",
      "all of his movies have one main thing in common- they don't skimp on the violence . \n",
      "in a verhoeven movie a guy doesn't get shot once in the chest- he gets shot many times all over the body . \n",
      "robocop takes place in detroit in the future . \n",
      "things are pretty bleak . \n",
      "the company ocp rules the city with an iron fist and is moving in on running the police force . \n",
      "peter weller plays a cop who goes on a mission with his partner , nancy allen . \n",
      "peter weller goes in an empty room alone and gets killed . \n",
      "meanwhile ocp has a failed demonstration of a ed-209 model robot crime fighter . \n",
      "someone comes up with the idea of having a cyborg robocop so they use any parts worth scalvaging from peter weller and make him into the robocop the movie is titled after . \n",
      "the plot of this movie generally runs at a decent pace . \n",
      "it drags a bit towards the end with robocop wanting to finish everybody off , but plot twists keep it exciting . \n",
      "the acting is bad from weller when he is human , but very good when he is robocop . \n",
      "nancy allen does a decent job . \n",
      "this movie doesn't leave itself open for a sequel exactly . \n",
      "but they made one anyway . \n",
      "enter . . . \n",
      "robocop 2 starring : peter weller and nancy allen directed by : irvin kershner \n",
      "detroit is in a more bleak state than it was in the first film . \n",
      "ocp is trying to develop an improved 'robocop 2' that will be more than a match for criminals . \n",
      "most of the police force is on strike due to ocp taking it over , leaving robocop to be the police force in the city . \n",
      "this film is pretty bizaare . \n",
      "it has a 13 year-old drug-lord who curses as much as eddie murphy , even more violence than the original , and some more humor . \n",
      "the plot of this movie is pretty large , almost too much so . \n",
      "there are three or four plot lines weaving around that don't really match up . \n",
      "the effects are still decent though . \n",
      "this sequel isn't more of the same really , just a darker version robocop dealing more with drugs than with armed criminals . \n",
      "if you liked the first one , watch this . \n",
      "but be dissapointed . \n",
      "so later comes . . . \n",
      "robocop 3 starring nancy allen and robert john burke \n",
      "ocp is now owned by a large japanese company . \n",
      "robocop joins a band of rebels to counter-attack the evil empire , i mean ocp . \n",
      "the japanese send a cyborg-ninja to deal with robocop . \n",
      "among other things . . . \n",
      "this movie is easier to follow than the last one . \n",
      "which means the plot isn't quite as crowded . \n",
      "this movie isn't anything special though . \n",
      "just more of robocop blasting things up in a more kid-friendly way . \n",
      "really not reccomended . \n",
      "\n",
      "         x         w    abs_xw        xw\n",
      "plot     5 -0.002187  0.010933 -0.010933\n",
      "this     9 -0.000987  0.008880 -0.008880\n",
      "and      7  0.001253  0.008773  0.008773\n",
      "the     27  0.000307  0.008280  0.008280\n",
      "he       4  0.001413  0.005653  0.005653\n",
      "on       4 -0.001200  0.004800 -0.004800\n",
      "bad      1 -0.004760  0.004760 -0.004760\n",
      "movie    8 -0.000453  0.003627 -0.003627\n",
      "though   2  0.001800  0.003600  0.003600\n",
      "is      12  0.000293  0.003520  0.003520 \n",
      "\n",
      "File cv041_22364.txt\n",
      "True sentiment:  -1 .  Predicted sentiment:  1.0 . Score:  0.00106665244463\n",
      "out of sight director steven sorderbergh baffles the hell out of us all in the limey , a cold , uninvolving , confusing new thriller . \n",
      "though the plot description may at first seem like it came from the pen of elmore leonard ( author of out of sight , as well as jackie brown , get shorty and pulp fiction ) , after you watch it , you realize that it's not nearly good enough . \n",
      "in an aggressively non-linear fashion , the limey ( li * mey , noun : an english gentleman ) tells the story of wilson ( terrence stamp ) , a british ex-con just released from a 9 year stint in prison for armed robbery . \n",
      "he has come to the us to seek vengeance for the death of his daughter jenny . \n",
      "he doesn't know much about the circumstances of her demise , all he has is a name : terry valentine . \n",
      "valentine was jenny's former boyfriend , a wealthy and corrupt record executive . \n",
      "he's played by peter fonda , in his first major role since the terrific ulee's gold in 1997 . \n",
      "seeking valentine's reclusive place of residence turns out to be no easy task for wilson . \n",
      "he finally finds the impressive abode high in the mountains and sneaks in just as valentine is having a big party . \n",
      "he winds up breaking his cover eventually , setting off valentine's head of security and valentine himself , who decides to run for it . \n",
      "what a mess . \n",
      "i have no problem when films refuse to be constricted by the linearity of time -- pulp fiction , which twisted time every which way , was a masterpiece -- but i do take exception to movies that decide to play around with it for no reason other than to confuse the viewer . \n",
      "the limey does exactly that . \n",
      "the plot is permeated with flashbacks , flash-forwards and what can only be described as random time-travel , without any evident purpose . \n",
      "there is no method to this movie's madness . \n",
      "it uses a fancy way to tell a story that would be better off told more conventionally and more comprehendably . \n",
      "the plot isn't particularly interesting in the first place : traditional , mildly hackneyed and not very involving . \n",
      "this is a sort of brooding film -- our protagonist doesn't speak much and the action sequences are done with an annoyingly perfunctory attitude . \n",
      "i felt like the director wasn't very interested in the proceedings himself , almost like he made this film for a paycheck . \n",
      "ditto for the editing , which seems to be deliberately sloppy and unpleasant . \n",
      "sixties icon terrence stamp manages to at least be menacing as the aging criminal . \n",
      "he's not much in the way of stature but he has a surprisingly imposing physical presence that works to his advantage here . \n",
      "peter fonda is an unbelievably underrated actor : he's shy , quiet but always effective . \n",
      "he's adept at conveying emotions through speech rather than expression : his feelings don't always show on his face by you can always tell what they are . \n",
      "this is basically a conventional thriller told in a pretentiously bizarre fashion . \n",
      "why soderbergh couldn't just parrot down and tell a story , i don't know , but what he does do certainly doesn't work . \n",
      "the result is a wild cornucopia of images that amount to precisely nil -- even the action scenes don't work . \n",
      "1999 may have signified the death of the traditional act one - act two - act three storyline , but obviously some movies have not yet transcended it . \n",
      "shall we go back to basics ? \n",
      "\n",
      "       x         w    abs_xw        xw\n",
      "and   10  0.001253  0.012533  0.012533\n",
      "he     8  0.001413  0.011307  0.011307\n",
      "the   28  0.000307  0.008587  0.008587\n",
      "no     4 -0.001787  0.007147 -0.007147\n",
      "plot   3 -0.002187  0.006560 -0.006560\n",
      "to    15 -0.000413  0.006200 -0.006200\n",
      "way    3  0.001773  0.005320  0.005320\n",
      "what   4  0.001213  0.004853  0.004853\n",
      "have   3 -0.001533  0.004600 -0.004600\n",
      "you    3  0.001467  0.004400  0.004400 \n",
      "\n",
      "File cv605_12730.txt\n",
      "True sentiment:  -1 .  Predicted sentiment:  1.0 . Score:  0.0272263036493\n",
      "a fullyloaded entertainment review : website coming soon ! \n",
      "hunter s . thompson's fear and loathing in las vegas , written in 1971 , is already an american classic ; not merely because it was an unadulterated journey through the post-psychedelia of the 1960's , but also because it ushered in a new form of journalism known as \" gonzo \" . \n",
      "soon after the book was written , hunter s . thomspon became the basis for a character in doonesbury known as uncle duke . \n",
      "and that is the problem with fear and loathing in las vegas ( the movie . ) \n",
      "although the book is one of my favorites , i now agree with thompson's own assessment of the novel as \" unfilmable . \" \n",
      "the main problem is that the book , although detailing wild drug use and tripped-out adventures , was not written on the road . \n",
      "it was written and edited by hunter s . thomspons , relatively sober , at his home . \n",
      "so although the book depicts drug use and still more drug use , it is told with a sense of aloofness and some humor . \n",
      "hunter s . thomspon , the narrator , while always drugged out , still has a grip on reality , and can still tell , with journalistic ability , what is going on around him . \n",
      "not so with the characters in fear and loathing in las vegas . \n",
      "despite name changes and one minor scene from the book dropped , the movie is an exact duplicate of the book , in terms of dialouge . \n",
      "unfortunately , this can get annoying , in the form of johnny depp's ( who plays thompson ) excessive narration . \n",
      "and the characters , while having vivid personalities , are played as 2-d characers on the screen : dr . raoul duke ( johnny depp , the name raoul duke was an alias of thompson's ) and his riding companion , dr . gonzo ( benicio del toro ) , who is a large samoan lawyer , have no personalities or emotions at all , save for stoned . \n",
      "the film is very faithful to the plot ( what there is ) of the book , in that dr . raoul duke is sent by sports illustrated to cover the mint 400 , and takes along not only his samoan lawyer but also car trunk full of drugs . \n",
      "from there , one set of stoned adventures after another happens , including one of the few good scenes in which depp , high on acid , watches all the people in the bar around him turn into , literally , lounge lizards . \n",
      "while the movie has its moments ( few ) , it is almost unwatchable at parts , especially towards the end . \n",
      "despite a lot of celebrity cameos , the stars never get off the ground . \n",
      "the set decoration and costumes are great , and probably deserve an oscar nomination . \n",
      "however , as we all know , it's not sets or effects that make a movie , its the characters ; and quite frankly , these characters are no good . \n",
      "\n",
      "        x         w    abs_xw        xw\n",
      "and    16  0.001253  0.020053  0.020053\n",
      "the    34  0.000307  0.010427  0.010427\n",
      "s       4 -0.001547  0.006187 -0.006187\n",
      "on      5 -0.001200  0.006000 -0.006000\n",
      "also    2  0.002173  0.004347  0.004347\n",
      "book    7  0.000613  0.004293  0.004293\n",
      "an      5 -0.000840  0.004200 -0.004200\n",
      "no      2 -0.001787  0.003573 -0.003573\n",
      "is     12  0.000293  0.003520  0.003520\n",
      "while   3  0.001173  0.003520  0.003520 \n",
      "\n",
      "File cv893_26269.txt\n",
      "True sentiment:  1 .  Predicted sentiment:  -1.0 . Score:  -0.00251996640045\n",
      "let me open this one with a confession : i love cop movies . \n",
      "i adore them with such an unwavering , near foolish passion that i was actually one of the first in line to see 1998's one tough cop starring the smirkiest of the baldwin brothers , steven . \n",
      "was it any good ? \n",
      "well , just about as good as baldwin's steven seagal impression ( for those i've confused the answer is a resounding no ! ) . \n",
      "now let me clarify , i don't love just any cop movie ; i'm not too keen on the smug buddy cop flicks that were so in vogue in the 80's and now slowly ( gulp ) returning thanks to jackie chan and chris tucker . \n",
      "i'm partial to those gritty , earthy cop dramas like serpico , donnie brasco , and the french connection . \n",
      "problem is these films come along just about as often as george w . bush makes a cogent point so whenever a \" true \" cop movie does hit multiplexes , i come running towards it like an eager puppy ? ? ? and inevitably i leave , head bowed , tripping over my squiggly tail . \n",
      "so what is it about cop movies that transform me into a moron willing to shell out $7 . 50 for a movie called one tough cop ( which actually sounds a lot like a wiseguy spin-off , no ? ) ? \n",
      "truth is i'm a sucker for those barren streets , that gritty , graffiti strewn modernist jungle where dangerous criminals lurk and cops , like modern day knights , must shield us from their iniquitous clutches . \n",
      "yes it's all very corny and awfully boys clubish , which , i'm guessing , is precisely where these feelings come from . \n",
      "as little tots us boys are brought up to create our own wars with gi joe's , enact cops and robbers or cowboy and indian fantasies , etc . the best gritty cop films tap into those gleeful adolescent fantasies , and heighten them , turning it all into an existential nightmare of thin moral lines and psychological chaos . \n",
      "as an adult my tastes haven't changed all that drastically ; sure i can get a cerebral high from the emotional complexities of a krzysztof kieslowski film , but i still delight at the promise a \" guy \" film could potentially bring . \n",
      "now what if it's the cops who're the corrupt ones ? \n",
      "ooh that one really gets me going , these protectors using all that at their service , taking advantage of others for their own gain . \n",
      "throw a courtroom drama ( yech ! ) \n",
      "into the pot and you have the tremendously absorbing sidney lumet film prince of the city . \n",
      "how absorbing is prince of the city , you ask ? \n",
      "so damn absorbing that when the film shifts from gritty undercover cop drama to courtroom drama ( a sub-genre i loath the way kristy swanson must loath sarah michelle gellar ) i remained completely and utterly involved . \n",
      "the film stars treat williams , who gives the kind of performance that should have skyrocketed his career up to the level that brad pitt currently resides at . \n",
      "since i was hatched a mere two years prior to the release of prince of the city , i cannot provide an answer as to why it didn't . \n",
      " ( though judging from his recent roles in tripe like deep rising and dead heat i would guess that it probably has something to do with quite a few bad career moves ) . \n",
      "like mickey rourke and eric roberts ( both of whom are currently doing time in direct to video purgatory ) williams is a talented leading man who didn't make it to where he should have . \n",
      " ( if there was any justice in this dog gone world keanu reeves would be the one banished to direct to video hell ) . \n",
      "here , the actor is given a role not all that dissimilar from al pacino's in serpico ( also directed by lumet ) . \n",
      "williams plays danny ciello , a member of an elite squad of police officers who take down drug dealers on the gritty pre-juliani streets of new york . \n",
      "they work without uniforms or hours much like paid vigilantes . \n",
      "and that analogy is more than apt seeing as how the bunch are kinda sorta forced into using corrupt means to get the job done . \n",
      "the squad relies entirely on informants ( low level junkies ) to gain access to the big wigs , and in order to obtain the info they seek , they sometimes have to provide the junkies with the junk of their choice . \n",
      "danny has done this , and though he's a fairly honorable guy , who took the job in order to do good , his intentions have gotten a bit skewed along the way . \n",
      "he seeks redemption and concludes that the best way to go about getting it is by talking to internal affairs who then convince him to go undercover as a \" double agent \" . \n",
      "he does so with one condition : he will not rat on his partners . \n",
      "right away we know that that is exactly what he'll have to do . \n",
      "and watching the movie build to that is thrilling in the way that any great character driven story can be . \n",
      "undercover cop films are almost innately exciting , even the kind that pull out all the cliches ( like in too deep ) . \n",
      "how can one not be moved by the issues of betrayal , ambiguity , and the danger in putting oneself in such a risky position ? \n",
      "prince of the city takes all this and charges it with a perceptive , hard boiled script and taut unintrusive direction by old pro sidney lumet , as well as a handful of intense performances , most notably by treat . \n",
      "lumet's greatest contribution to the movie is how he lays back , and lets the camera soak up the action . \n",
      "his scenes transition rapidly from one to the next building layers of tension , and he rarely holds for reaction shots , instead he clicks away at precisely the moment a character stops yakking . \n",
      "lumet's worst tendency is one that many film makers ( especially oliver stone ) tend to overuse : he relies a little too heavily on actors shouting their lines at each other presumably in order to ratchet up the tension . \n",
      "this isn't as much a problem here as it was in night falls on manhattan , lumet's most recent cop \\ lawyer endeavor , a movie that nearly gave me an ulcer from just watching . \n",
      "though of late lumet has made several really awful career choices ( maybe he and treat should get together for a drink or something ) , the most recent a completely unnecessary remake of gloria , here he was on , sustaining tension in a film that runs for 167 minutes with not one dull spot . \n",
      "some of the formula pictures i like the best are the most simplistic . \n",
      "hollywood often forgets how easy it is to make these kinds of pictures even tolerable . \n",
      "of course cop movies aren't popular anymore ( yes buddy cop flicks are popular but those are more like extended sitcoms with gunfire amid the punch lines ) , but just look at the average hollywood action movies , flicks so bloated and full of unnecessary fat like the boring love interest that serves no purpose other than to stall the action and demonstrate that our hero isn't gay , the lame comic relief character who's never funny , and on and on . \n",
      "for all its moral ambiguities prince of the city is a back to the basics cop \\ courtroom drama that leaves out all the crap and really thrills . \n",
      "it focuses completely on its story , and its characters all of whom are completely believable from the cops to the lawyers to the junkies . \n",
      "there's a great scene early on where an irate williams shouts at an ia officer and looks as if he's about to go into one of those corny braveheart inspirational speeches , but instead he breaks down in front of the cop and begins sobbing . \n",
      "this is a scene that would never make it into a michael bay film ( even though bruce willis sacrifices his life at the end of armageddon , not one damn tear ) it would be cut out because the director would say oh that makes him too human , excuse me , weak . \n",
      "in the very next scene danny speaks in a hushed raspiness , straining to get the words out . \n",
      "he's lost his voice . \n",
      "does that little itty bitty detail matter ? \n",
      "maybe not to most , but it sure made my night . \n",
      "and in a sprawling epic of corruption , greed , betrayal and violence it's nice to see that someone is paying attention to the little things . \n",
      "\n",
      "       x         w    abs_xw        xw\n",
      "and   35  0.001253  0.043866  0.043866\n",
      "the   70  0.000307  0.021466  0.021466\n",
      "to    39 -0.000413  0.016120 -0.016120\n",
      "he    11  0.001413  0.015546  0.015546\n",
      "on    11 -0.001200  0.013200 -0.013200\n",
      "all   10 -0.001040  0.010400 -0.010400\n",
      "most   5  0.001960  0.009800  0.009800\n",
      "have   6 -0.001533  0.009200 -0.009200\n",
      "at     8 -0.001133  0.009067 -0.009067\n",
      "like  11 -0.000800  0.008800 -0.008800 \n",
      "\n",
      "File cv828_19831.txt\n",
      "True sentiment:  1 .  Predicted sentiment:  -1.0 . Score:  -0.0109331875575\n",
      "all those who were offended by there's something about mary should not tread in the waters of american pie , a gratuitously sexual rollercoaster ride of raunch . \n",
      "and while this uproariously funny , gross-out summer movie is basically an exercise in bad taste , it also demonstrates a surprising sweetness in the end . \n",
      "american pie is the latest entry to the offensive teenage-targeted fare this season . \n",
      "i must question the studio that releases a movie like this . \n",
      "the producers are aiming these films at the 15-24 crowd , but while pushing the limit of possible sex and violence in an r-rated film . \n",
      "some have even been narrowly avoiding the dreaded nc-17 , a rating the mpaa gives when the adult content surpasses even that of a restricted movie . \n",
      "american pie was threatened with an nc-17 , but after snipping a few scenes from the finished product , it was given an r . but at least this new addition is not as careless and unforgiving as something like south park . \n",
      "it is sick , perverse , and ultimately disgusting - not to mention extremely funny . \n",
      "american pie is about four desperate teenagers who make a pact to lose their virginity by prom night . \n",
      "jim ( jason biggs ) , kevin ( thomas ian nicholas ) , oz ( chris klein ) and finch ( eddie kaye thomas ) are a quartet of high school students in their senior year who think sex is something they must experience to be successful in their lives ahead . \n",
      "jim inquires what it feels like when you reach third base , to which one of his friends replies : `like warm apple pie . . . ' . \n",
      "of course , this prompts the scene unfairly exposed in the trailers , in which jim and a freshly baked pie have a very intimate moment in the kitchen corner . \n",
      "there are many moments , such as this , in which director paul weitz uses a game-plan similar to the one frequently displayed in mary : to cause the audience to break down laughing in disgusting disbelief . \n",
      "american pie is a hard-fought effort that has replenishing rewards if you manage to stick with it . \n",
      "i loved the young cast in this movie . \n",
      "consider the subject matter that these actors have dealt with , obviously suggestiveness never rivaled in any of their previous projects , and you should appreciate their performances . \n",
      "biggs is more than enjoyable , and chris klein ( who recently played a similar jock in election ) is obviously a young talent on the rise . \n",
      "but in a teen-dominated movie , the best performance comes from comedian eugene levy ( of television's sctv ) , who is unexpectedly brilliant as jim's uneasy father . \n",
      "scenes in which the familiar father-son conversations are brought to interesting new levels are the funniest moments to be found in american pie . \n",
      "after levy discovers jim's new use for apple pie , he tells him , `i did a fare share of that sort of thing when i was your age . \n",
      "but i never used baked goods . ' \n",
      "it is performances such as his , and bill murray's in rushmore , that are often unfairly dismissed when awards are handed out . \n",
      "there are some less-than-original aspects of the film . \n",
      "a bathroom incident involving finch and a bottle of ex-lax is something we've seen before . \n",
      "if weitz is planning to take after the farrelly brothers , then this is a decidedly unwise move : the same prank was pulled in the brother's dumb and dumber , and used to greater effect . \n",
      "secondly , the characters are forced into a half-hearted climax that seems more artificial than amusing . \n",
      "but at one point , in which we discover that american pie does have a heart of it's own , biggs asks his buddies why he's going through so much pressure for something that's `not very important anyway . ' \n",
      "and so we realize that while the film is expressive about a subject that is hardly appropriate , it still manages to be mature . \n",
      "in it's own way . \n",
      "\n",
      "           x         w    abs_xw        xw\n",
      "and       13  0.001253  0.016293  0.016293\n",
      "the       29  0.000307  0.008893  0.008893\n",
      "american   7  0.001133  0.007933  0.007933\n",
      "this       8 -0.000987  0.007893 -0.007893\n",
      "have       4 -0.001533  0.006133 -0.006133\n",
      "to        14 -0.000413  0.005787 -0.005787\n",
      "bad        1 -0.004760  0.004760 -0.004760\n",
      "is        16  0.000293  0.004693  0.004693\n",
      "you        3  0.001467  0.004400  0.004400\n",
      "but        6 -0.000680  0.004080 -0.004080 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for bag in bag_test:\n",
    "    score = dotProduct(w_opt, bag)\n",
    "    scores.append(score)\n",
    "\n",
    "score_df=pd.DataFrame(zip(scores,label_test))\n",
    "score_df.columns=('score','label')\n",
    "score_df['prediction']=np.sign(score_df['score'])\n",
    "score_df['err']= map(lambda x:int(x),score_df['label']!=score_df['prediction'])\n",
    "wrong = score_df[score_df.err==1]\n",
    "wrong_name = []\n",
    "for w in wrong.iterrows():\n",
    "    if int(w[1].label)==1:\n",
    "        name = \"data/pos/\"\n",
    "    else:\n",
    "        name = \"data/neg/\"\n",
    "    name+=ids_test[w[0]]\n",
    "    wrong_name.append(name)\n",
    "wrong['name']=wrong_name\n",
    "for i in wrong.index[:5]:\n",
    "    example_analysis(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2. [Optional] Try to get the best performance possible by generating lots of new features, changing the pre-processing, or any other method you want, so long as you are using the same core SVM model. Describe what you tried, and how much improvement each thing brought to the model. To get you thinking on features, here are some basic ideas of varying quality: 1) how many words are in the review? 2) How many “negative” words are there? (You’d have to construct or find a list of negative words.) 3) Word n-gram features: Instead of single-word features, you can make every pair of consecutive words a feature. 4) Character n-gram features: Ignore word boundaries and make every sequence of n characters into a feature (this will be a lot). 5) Adding an extra feature whenever a word is preceded by “not”. For example “not amazing” becomes its own feature. 6) Do we really need to eliminate those funny characters in the data loading phase? Might there be useful signal there? 7) Use tf-idf instead of raw word counts. The tf-idf is calculated as $$tfidf(f_i)=\\frac{FF_i}{log(DF_i)}$$\n",
    "where $FF_i$ is the feature frequency of feature $f_i$ and $DF_i$ is the number of document containing $f_i$. In this way we increase the weight of rare words. Sometimes this scheme helps, sometimes it makes things worse. You could try using both! [Extra credit points will be awarded in proportion to how much improvement you achieve.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_bigram(lst):\n",
    "    pre = \"START\"\n",
    "    bigrams = []\n",
    "    for word in lst:\n",
    "        bigram = str(pre) + \" \" + str(word)\n",
    "        bigrams.append(bigram)\n",
    "        pre = word\n",
    "    return bigrams + lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lst2bigram(lst):\n",
    "    bags = map(lambda x:collections.Counter(x[:-2]),lst)\n",
    "    labels = map(lambda x:x[-2],lst)\n",
    "    ids = map(lambda x:x[-1],lst)\n",
    "    return bags, labels, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bi_train = map(lambda x:add_bigram(x), train)\n",
    "bi_test = map(lambda x:add_bigram(x), test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bi_bag_train,bi_label_train,ids_train=lst2bag(bi_train)\n",
    "bi_bag_test,bi_label_test,bi_ids_test=lst2bag(bi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after removing stopwords.\n",
      "443763\n",
      "implemented_pegasos function took 39822.189 ms\n",
      "Percent Error: 0.172000 +/- 0.016877\n"
     ]
    }
   ],
   "source": [
    "print('Prediction after removing stopwords.')\n",
    "evaluate_model(bi_bag_train,bi_label_train,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
