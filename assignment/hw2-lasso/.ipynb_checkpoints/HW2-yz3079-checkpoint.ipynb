{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ is the \"design matrix\" with M\\*D(150*75)\n",
    "\n",
    "w\\_true is the true weight vector $w_{true} \\in\\mathbf R^{d\\times 1}$\n",
    "\n",
    "y is the response with M\\*1\n",
    "\n",
    "Dataset is split by taking 80 points for training, the next 2-00 points for validation and the last 50 points for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 150\n",
    "d = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1738)\n",
    "### Design Matrix ###\n",
    "X = np.random.rand(m,d)\n",
    "### Weight Vector ###\n",
    "w_true = np.zeros(d)\n",
    "w_true[:10] = (np.random.choice(2,10)-0.5)*2\n",
    "### Response ###\n",
    "sigma = 0.1 \n",
    "mu = 0\n",
    "epsilon = sigma*np.random.randn(m) + mu\n",
    "y = np.dot(X, w_true)+epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X[:80,:]\n",
    "X_validation = X[80:100,:]\n",
    "X_test = X[-50:150,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = y[:80]\n",
    "y_validation = y[80:100]\n",
    "y_test = y[-50:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run ridge regression on this dataset. Choose the $\\lambda$ that minimizes the square loss on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1e-09, 0.029819016409124582)\n",
      "(1e-08, 0.029817999953792869)\n",
      "(1e-07, 0.029807937055520251)\n",
      "(1e-06, 0.02970870510534766)\n",
      "(1e-05, 0.029149523109953529)\n",
      "(0.0001, 0.026048744308054129)\n",
      "(0.001, 0.035628138031012738)\n",
      "(0.01, 0.1000448583973369)\n",
      "(0.1, 0.26339067097969077)\n",
      "(1, 0.3654767260665357)\n",
      "Best Lambda 0.0001\n",
      "Best Loss 0.0260487443081\n"
     ]
    }
   ],
   "source": [
    "def ridge(X, y, Lambda):\n",
    "    (N, D) = X.shape\n",
    "    def ridge_obj(theta):\n",
    "        return ((np.linalg.norm(np.dot(X, theta) - y))**2) / (2 * N) +\\\n",
    "            Lambda * (np.linalg.norm(theta))**2\n",
    "    return ridge_obj\n",
    "\n",
    "def compute_loss(X, y, theta):\n",
    "    (N, D) = X.shape\n",
    "    return ((np.linalg.norm(np.dot(X, theta) - y))**2) / (2 * N)\n",
    "\n",
    "def ridge_regression(Lambda=0):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        lambda - if not given loop through differen lambdas and find the best\n",
    "    Returns:\n",
    "        theta_opt - the optimization of parameter vector, 1D numpy array of size (num_features)\n",
    "        lambda_opt - the history optimization of parameter, 1D numpy array of size (num_lambdas)\n",
    "        loss_opt - the history of regularized loss value, 1D numpy array of size (num_lambdas)\n",
    "    \"\"\"\n",
    "    (N, D) = X.shape\n",
    "    w = np.random.rand(D, 1)\n",
    "    Lambda_history = []\n",
    "    loss_history = []\n",
    "    t = 0\n",
    "    loss_min=lambda_min=w_min=np.nan\n",
    "    if (Lambda==0):\n",
    "        for i in range(-9, 1):\n",
    "            Lambda = 10**i\n",
    "            Lambda_history.append(Lambda)\n",
    "            w_opt = minimize(ridge(X_train, y_train, Lambda), w)\n",
    "            loss = compute_loss(X_validation, y_validation, w_opt.x)\n",
    "            loss_history.append(loss)\n",
    "            print(Lambda, loss)\n",
    "            if (t==0 or loss<loss_min):\n",
    "                loss_min = loss\n",
    "                lambda_min = Lambda\n",
    "                w_min = w_opt.x.copy()\n",
    "                t=t+1\n",
    "    else:\n",
    "        w_opt = minimize(ridge(X_train, y_train,Lambda), w)\n",
    "        loss_opt = compute_loss(X_validation, y_validation, w_opt.x)\n",
    "        return w_opt.x.copy(), Lambda, loss_opt\n",
    "    \n",
    "    print \"Best Lambda\",lambda_min\n",
    "    print \"Best Loss\", loss_min\n",
    "    return w_min, Lambda_history, loss_history\n",
    "w_min, Lambda_hist, loss_hist = ridge_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113154dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Lambda_hist, loss_hist)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Loss')\n",
    "plt.xscale('log')\n",
    "plt.title('Ridge Regression: Loss vs. Lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With tolerence of 0.0\n",
      "True Value Zero estimated as Nonzero: 65\n",
      "True Value Nonzero estimated as Zero: 0\n",
      "True Value Zero estimated as Zero: 0\n",
      "True Value Nonzero estimated as Nonzero: 10\n",
      "With tolerence of 0.05\n",
      "True Value Zero estimated as Nonzero: 41\n",
      "True Value Nonzero estimated as Zero: 0\n",
      "True Value Zero estimated as Zero: 24\n",
      "True Value Nonzero estimated as Nonzero: 10\n"
     ]
    }
   ],
   "source": [
    "### Choose 1e-4 as lambda, test model coefficients ###\n",
    "Lambda = 10**-4\n",
    "w_min, Lambda, loss_hist = ridge_regression(Lambda)\n",
    "\n",
    "### Coefficient Report under tolerence=0.0 ###\n",
    "\n",
    "tolerance = 0.0\n",
    "false_nonzeros = sum((w_true==0) & (abs(w_min)>=tolerance))\n",
    "false_zeros = sum((w_true!=0) & (abs(w_min)<tolerance))\n",
    "true_nonzeros = sum((w_true!=0) & (abs(w_min)>=tolerance))\n",
    "true_zeros = sum((w_true==0) & (abs(w_min)<tolerance))\n",
    "print \"With tolerence of {0}\".format(tolerance)\n",
    "print \"True Value Zero estimated as Nonzero: {0}\".format(false_nonzeros)\n",
    "print \"True Value Nonzero estimated as Zero: {0}\".format(false_zeros)\n",
    "print \"True Value Zero estimated as Zero: {0}\".format(true_zeros)\n",
    "print \"True Value Nonzero estimated as Nonzero: {0}\".format(true_nonzeros)\n",
    "\n",
    "### Coefficient Report under tolerence=0.05 ###\n",
    "\n",
    "tolerance = 0.05\n",
    "false_nonzeros = sum((w_true==0) & (abs(w_min)>=tolerance))\n",
    "false_zeros = sum((w_true!=0) & (abs(w_min)<tolerance))\n",
    "true_nonzeros = sum((w_true!=0) & (abs(w_min)>=tolerance))\n",
    "true_zeros = sum((w_true==0) & (abs(w_min)<tolerance))\n",
    "print \"With tolerence of {0}\".format(tolerance)\n",
    "print \"True Value Zero estimated as Nonzero: {0}\".format(false_nonzeros)\n",
    "print \"True Value Nonzero estimated as Zero: {0}\".format(false_zeros)\n",
    "print \"True Value Zero estimated as Zero: {0}\".format(true_zeros)\n",
    "print \"True Value Nonzero estimated as Nonzero: {0}\".format(true_nonzeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coordinate Descent for Lasso (a.k.a. The Shooting Algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Experiment with the Shooting Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Write a function that computes the Lasso solution for a given $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged: True Iterations: 69\n",
      "Converged: True Iterations: 69\n",
      "Converged: True Iterations: 69\n",
      "Converged: True Iterations: 69\n",
      "1 loop, best of 3: 1.14 s per loop\n"
     ]
    }
   ],
   "source": [
    "def soft(a, delta):\n",
    "    return np.sign(a)*max( abs(a) - delta, 0)\n",
    "\n",
    "def lasso_shooting(X, y, Lambda=10, tolerance=1e-4):\n",
    "    (N, D) = X.shape\n",
    "    maxIt = 1000\n",
    "    ### Shooting Algo ###\n",
    "    it = 1\n",
    "    converged = False\n",
    "    ### Initialize w ###\n",
    "    w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X) + np.identity(D)), X.T), y)\n",
    "    while (not converged) & (it<maxIt):\n",
    "        w_old = w.copy()\n",
    "        for j in range(D):\n",
    "            aj=cj=0\n",
    "            for i in range(1,N):\n",
    "                aj=aj+2*X[i,j]**2\n",
    "                cj=cj+2*X[i,j]*(y[i]-np.dot(w,X[i,:])+ w[j]*X[i,j])\n",
    "\n",
    "            w[j] = soft(cj/aj,Lambda/aj)\n",
    "        it = it + 1\n",
    "        converged = (sum(abs(w-w_old)) < tolerance)\n",
    "    print \"Converged:\",converged,\"Iterations:\",it\n",
    "    return w, converged\n",
    "\n",
    "### sanity test ###\n",
    "%timeit lasso_shooting(X_train, y_train, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeme(func,iterations=1,*args,**kwargs):\n",
    "    \"\"\"\n",
    "    Timer wrapper.  Runs a given function, with arguments,\n",
    "    100 times and displays the average time per run.  \n",
    "    \"\"\"\n",
    "    def wrapper(func, *args, **kwargs):\n",
    "        def wrapped():\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapped\n",
    "    wrapped = wrapper(func,*args,**kwargs)\n",
    "    run_time = float(timeit.timeit(wrapped, number=iterations))/iterations\n",
    "    print \"Avg time to run %s after %i trials: %i seconds per trial\" %(func,iterations,run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Searching\n",
      "Lambda 0.01\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.018977868643\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.0360159498232\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.06835059648\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.129714864167\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.246171165321\n",
      "Converged: True Iterations: 927\n",
      "Lambda 0.467180403917\n",
      "Converged: True Iterations: 458\n",
      "Lambda 0.886608833814\n",
      "Converged: True Iterations: 254\n",
      "Lambda 1.68259459859\n",
      "Converged: True Iterations: 186\n",
      "Lambda 3.19320592715\n",
      "Converged: True Iterations: 169\n",
      "Lambda 6.06002426356\n",
      "Converged: True Iterations: 142\n",
      "Lambda 11.5006344447\n",
      "Converged: True Iterations: 54\n",
      "Lambda 21.8257529804\n",
      "Converged: True Iterations: 47\n",
      "Lambda 41.4206273097\n",
      "Converged: True Iterations: 17\n",
      "Lambda 78.6075224196\n",
      "Converged: True Iterations: 7\n",
      "Converged: True Iterations: 775\n",
      "Best Lambda: 0.246171165321\n",
      "Square Loss on Test Data: 0.008095764033\n",
      "Start Searching\n",
      "Lambda 0.01\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.018977868643\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.0360159498232\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.06835059648\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.129714864167\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.246171165321\n",
      "Converged: True Iterations: 927\n",
      "Lambda 0.467180403917\n",
      "Converged: True Iterations: 458\n",
      "Lambda 0.886608833814\n",
      "Converged: True Iterations: 254\n",
      "Lambda 1.68259459859\n",
      "Converged: True Iterations: 186\n",
      "Lambda 3.19320592715\n",
      "Converged: True Iterations: 169\n",
      "Lambda 6.06002426356\n",
      "Converged: True Iterations: 142\n",
      "Lambda 11.5006344447\n",
      "Converged: True Iterations: 54\n",
      "Lambda 21.8257529804\n",
      "Converged: True Iterations: 47\n",
      "Lambda 41.4206273097\n",
      "Converged: True Iterations: 17\n",
      "Lambda 78.6075224196\n",
      "Converged: True Iterations: 7\n",
      "Converged: True Iterations: 775\n",
      "Best Lambda: 0.246171165321\n",
      "Square Loss on Test Data: 0.008095764033\n",
      "Start Searching\n",
      "Lambda 0.01\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.018977868643\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.0360159498232\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.06835059648\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.129714864167\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.246171165321\n",
      "Converged: True Iterations: 927\n",
      "Lambda 0.467180403917\n",
      "Converged: True Iterations: 458\n",
      "Lambda 0.886608833814\n",
      "Converged: True Iterations: 254\n",
      "Lambda 1.68259459859\n",
      "Converged: True Iterations: 186\n",
      "Lambda 3.19320592715\n",
      "Converged: True Iterations: 169\n",
      "Lambda 6.06002426356\n",
      "Converged: True Iterations: 142\n",
      "Lambda 11.5006344447\n",
      "Converged: True Iterations: 54\n",
      "Lambda 21.8257529804\n",
      "Converged: True Iterations: 47\n",
      "Lambda 41.4206273097\n",
      "Converged: True Iterations: 17\n",
      "Lambda 78.6075224196\n",
      "Converged: True Iterations: 7\n",
      "Converged: True Iterations: 775\n",
      "Best Lambda: 0.246171165321\n",
      "Square Loss on Test Data: 0.008095764033\n",
      "Avg time to run <function lambda_search at 0x115e98758> after 3 trials: 114 seconds per trial\n"
     ]
    }
   ],
   "source": [
    "def lambda_search():\n",
    "    t=0\n",
    "    Lambdas=[]\n",
    "    loss_hist=[]\n",
    "    loss_min = lambda_min=w_opt=np.nan\n",
    "    print \"Start Searching\"\n",
    "    Lambda_max = np.linalg.norm(np.dot(X.T,y),np.inf)\n",
    "    log_lambda_max = np.log10(Lambda_max)\n",
    "    for i in np.linspace(-2, log_lambda_max, 15):\n",
    "        Lambda = 10**i\n",
    "        print \"Lambda\",Lambda\n",
    "        w, converged = lasso_shooting(X_train, y_train, Lambda)\n",
    "        Lambdas.append(Lambda)\n",
    "        loss = compute_loss(X_validation, y_validation, w)\n",
    "        loss_hist.append(loss)\n",
    "        if t==0:\n",
    "            loss_min = loss\n",
    "            lambda_min = Lambda\n",
    "            w_min = w.copy()  \n",
    "        elif converged:\n",
    "            if loss<=loss_min:\n",
    "                loss_min = loss\n",
    "                lambda_min = Lambda\n",
    "                w_min = w.copy()  \n",
    "        t=t+1\n",
    "    best_lost = lasso_shooting(X_test, y_test, lambda_min)\n",
    "    print \"Best Lambda:\",lambda_min\n",
    "    print \"Square Loss on Test Data:\", loss_min\n",
    "    \n",
    "    return w_opt,Lambdas,loss_hist\n",
    "\n",
    "\n",
    "timeme(lambda_search,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Searching\n",
      "Lambda 0.01\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.018977868643\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.0360159498232\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.06835059648\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.129714864167\n",
      "Converged: False Iterations: 1000\n",
      "Lambda 0.246171165321\n",
      "Converged: True Iterations: 927\n",
      "Lambda 0.467180403917\n",
      "Converged: True Iterations: 458\n",
      "Lambda 0.886608833814\n",
      "Converged: True Iterations: 254\n",
      "Lambda 1.68259459859\n",
      "Converged: True Iterations: 186\n",
      "Lambda 3.19320592715\n",
      "Converged: True Iterations: 169\n",
      "Lambda 6.06002426356\n",
      "Converged: True Iterations: 142\n",
      "Lambda 11.5006344447\n",
      "Converged: True Iterations: 54\n",
      "Lambda 21.8257529804\n",
      "Converged: True Iterations: 47\n",
      "Lambda 41.4206273097\n",
      "Converged: True Iterations: 17\n",
      "Lambda 78.6075224196\n",
      "Converged: True Iterations: 7\n",
      "Converged: True Iterations: 775\n",
      "Best Lambda: 0.246171165321\n",
      "Square Loss on Test Data: 0.008095764033\n"
     ]
    }
   ],
   "source": [
    "w_opt,Lambdas,loss_hist_lasso = lambda_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEdCAYAAAD5KpvoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVeUkYWZmVTlJmJlZVU4SZmZW1f8HQHSfuRzl7KUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1131547d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Lambdas,loss_hist_lasso)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Loss')\n",
    "plt.xscale('log')\n",
    "plt.title('Lasso Shooting: Loss vs. Lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Analyze the sparsity of your solution, reporting how many components with true value zero have been estimated to be non-zero, and vice-versa.\n",
    "> There are 3 cases where true value zero were estimated as nonzero. Vice versa never happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged: True Iterations: 927\n",
      "With tolerence of 0.05\n",
      "True Value Zero estimated as Nonzero: 3\n",
      "True Value Nonzero estimated as Zero: 0\n",
      "True Value Zero estimated as Zero: 62\n",
      "True Value Nonzero estimated as Nonzero: 10\n"
     ]
    }
   ],
   "source": [
    "### Choose 0.246171165321 as lambda, test model coefficients ###\n",
    "Lambda = 0.246171165321\n",
    "w_opt, _ = lasso_shooting(X_train, y_train, Lambda)\n",
    "\n",
    "### Coefficient Report under tolerence=0.05 ###\n",
    "tolerance = 0.05\n",
    "false_nonzeros = sum((w_true==0) & (abs(w_opt)>=tolerance))\n",
    "false_zeros = sum((w_true!=0) & (abs(w_opt)<tolerance))\n",
    "true_nonzeros = sum((w_true!=0) & (abs(w_opt)>=tolerance))\n",
    "true_zeros = sum((w_true==0) & (abs(w_opt)<tolerance))\n",
    "print \"With tolerence of {0}\".format(tolerance)\n",
    "print \"True Value Zero estimated as Nonzero: {0}\".format(false_nonzeros)\n",
    "print \"True Value Nonzero estimated as Zero: {0}\".format(false_zeros)\n",
    "print \"True Value Zero estimated as Zero: {0}\".format(true_zeros)\n",
    "print \"True Value Nonzero estimated as Nonzero: {0}\".format(true_nonzeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Implement the homotopy method described above. Compare the runtime for computing the full regularization path (for the same set of $\\lambda$’s you tried in the first question above) using the homotopy method compared to the basic shooting algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def soft(a, delta):\n",
    "    return np.sign(a)*max( abs(a) - delta, 0)\n",
    "\n",
    "def homotopy_lasso_shooting(X, y, Lambda=1, tolerance=1e-4, w='undefined'):\n",
    "    (N, D) = X.shape\n",
    "    maxIt = 1000\n",
    "    ### Shooting Algo ###\n",
    "    it = 1\n",
    "    converged = False\n",
    "    ### Initialize w ###\n",
    "    if w=='undefined':\n",
    "        w=np.zeros(D)\n",
    "    while (not converged) & (it<maxIt):\n",
    "        w_old = w.copy()\n",
    "        for j in range(D):\n",
    "            aj=cj=0\n",
    "            for i in range(1,N):\n",
    "                aj=aj+2*X[i,j]**2\n",
    "                cj=cj+2*X[i,j]*(y[i]-np.dot(w,X[i,:])+ w[j]*X[i,j])\n",
    "\n",
    "            w[j] = soft(cj/aj,Lambda/aj)\n",
    "        it = it + 1\n",
    "        converged = (sum(abs(w-w_old)) < tolerance)\n",
    "    print \"Converged:\",converged,\"Iterations:\",it\n",
    "    return w, converged\n",
    "#homotopy_lasso_shooting(X_train, y_train, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Searching\n",
      "Lambda = 78.6075224196. Warm Starting.....\n",
      "Converged: True Iterations: 3\n",
      "Lambda = 41.4206273097. Warm Starting.....\n",
      "Converged: True Iterations: 22\n",
      "Lambda = 21.8257529804. Warm Starting.....\n",
      "Converged: True Iterations: 44\n",
      "Lambda = 11.5006344447. Warm Starting.....\n",
      "Converged: True Iterations: 44\n",
      "Lambda = 6.06002426356. Warm Starting.....\n",
      "Converged: True Iterations: 140\n",
      "Lambda = 3.19320592715. Warm Starting.....\n",
      "Converged: True Iterations: 171\n",
      "Lambda = 1.68259459859. Warm Starting.....\n",
      "Converged: True Iterations: 165\n",
      "Lambda = 0.886608833814. Warm Starting.....\n",
      "Converged: True Iterations: 192\n",
      "Lambda = 0.467180403917. Warm Starting.....\n",
      "Converged: True Iterations: 302\n",
      "Lambda = 0.246171165321. Warm Starting.....\n",
      "Converged: True Iterations: 647\n",
      "Lambda = 0.129714864167. Warm Starting.....\n",
      "Converged: True Iterations: 875\n",
      "Lambda = 0.06835059648. Warm Starting.....\n",
      "Converged: False Iterations: 1000\n",
      "Lambda = 0.0360159498232. Warm Starting.....\n",
      "Converged: True Iterations: 886\n",
      "Lambda = 0.018977868643. Warm Starting.....\n",
      "Converged: False Iterations: 1000\n",
      "Lambda = 0.01. Warm Starting.....\n",
      "Converged: False Iterations: 1000\n",
      "Converged: False Iterations: 1000\n",
      "Best Lambda: 0.129714864167\n",
      "Square Loss on Test Data: 0.00802342043097\n",
      "Avg time to run <function homotopy_lambda_search at 0x115e1b398> after 1 trials: 105 seconds per trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:11: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "def homotopy_lambda_search():\n",
    "    t=0\n",
    "    Lambdas=[]\n",
    "    loss_hist=[]\n",
    "    loss_min = lambda_min=w_opt=np.nan\n",
    "    (N, D) = X.shape\n",
    "    Lambda_max = np.linalg.norm(np.dot(X.T,y),np.inf)\n",
    "    log_lambda_max = np.log10(Lambda_max)\n",
    "    print \"Start Searching\"\n",
    "    w_old = w = np.zeros(D)\n",
    "    for i in np.linspace(log_lambda_max, -2, 15):\n",
    "        Lambda = 10**i\n",
    "        print \"Lambda = {0}. Warm Starting.....\".format(Lambda)\n",
    "        \n",
    "        w_old = w\n",
    "        w, converged = homotopy_lasso_shooting(X_train, y_train, Lambda=Lambda, w=w_old)\n",
    "\n",
    "        Lambdas.append(Lambda)\n",
    "        loss = compute_loss(X_validation, y_validation, w)\n",
    "        loss_hist.append(loss)\n",
    "        if t==0:\n",
    "            loss_min = loss\n",
    "            lambda_min = Lambda\n",
    "            w_min = w.copy()  \n",
    "        elif converged:\n",
    "            if loss<=loss_min:\n",
    "                loss_min = loss\n",
    "                lambda_min = Lambda\n",
    "                w_min = w.copy()  \n",
    "        t=t+1\n",
    "    best_lost = homotopy_lasso_shooting(X_test, y_test, lambda_min)\n",
    "    print \"Best Lambda:\",lambda_min\n",
    "    print \"Square Loss on Test Data:\", loss_min\n",
    "    \n",
    "    return w_opt,Lambdas,loss_hist\n",
    "\n",
    "timeme(homotopy_lambda_search)\n",
    "#%timeit w_lasso,lambdas_lasso,loss_hist_lasso = homotopy_lambda_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Implement the matrix expressions and measure the speedup to compute the regularization path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soft(a, delta):\n",
    "    return np.sign(a)*max( abs(a) - delta, 0)\n",
    "\n",
    "def vect_homotopy_lasso_shooting(X, y, Lambda=1, tolerance=1e-4, w='undefined'):\n",
    "    (N, D) = X.shape\n",
    "    maxIt = 1000\n",
    "    ### Shooting Algo ###\n",
    "    it = 1\n",
    "    converged = False\n",
    "    ### Initialize w ###\n",
    "    if w=='undefined':\n",
    "        w=np.zeros(D)\n",
    "    while (not converged) & (it<maxIt):\n",
    "        w_old = w.copy()\n",
    "        for j in range(D):\n",
    "            aj = 2*np.dot(X[:,j].T,X[:,j]) \n",
    "            cj = 2*(X[:,j].dot(y) - (w.T.dot(X.T)).dot(X[:,j]) + w[j]*(X[:,j].T.dot(X[:,j])))\n",
    "            w[j] = soft(cj/aj,Lambda/aj)\n",
    "        it = it + 1\n",
    "        converged = (sum(abs(w-w_old)) < tolerance)\n",
    "    print \"Converged:\",converged,\"Iterations:\",it\n",
    "    return w, converged\n",
    "#homotopy_lasso_shooting(X_train, y_train, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Searching\n",
      "Lambda = 78.6075224196. Warm Starting.....\n",
      "Converged: True Iterations: 3\n",
      "Lambda = 41.4206273097. Warm Starting.....\n",
      "Converged: True Iterations: 22\n",
      "Lambda = 21.8257529804. Warm Starting.....\n",
      "Converged: True Iterations: 44\n",
      "Lambda = 11.5006344447. Warm Starting.....\n",
      "Converged: True Iterations: 44\n",
      "Lambda = 6.06002426356. Warm Starting.....\n",
      "Converged: True Iterations: 146\n",
      "Lambda = 3.19320592715. Warm Starting.....\n",
      "Converged: True Iterations: 175\n",
      "Lambda = 1.68259459859. Warm Starting.....\n",
      "Converged: True Iterations: 168\n",
      "Lambda = 0.886608833814. Warm Starting.....\n",
      "Converged: True Iterations: 196\n",
      "Lambda = 0.467180403917. Warm Starting.....\n",
      "Converged: True Iterations: 313\n",
      "Lambda = 0.246171165321. Warm Starting.....\n",
      "Converged: True Iterations: 666\n",
      "Lambda = 0.129714864167. Warm Starting.....\n",
      "Converged: True Iterations: 884\n",
      "Lambda = 0.06835059648. Warm Starting.....\n",
      "Converged: False Iterations: 1000\n",
      "Lambda = 0.0360159498232. Warm Starting.....\n",
      "Converged: True Iterations: 803\n",
      "Lambda = 0.018977868643. Warm Starting.....\n",
      "Converged: True Iterations: 631\n",
      "Lambda = 0.01. Warm Starting.....\n",
      "Converged: False Iterations: 1000\n",
      "Converged: False Iterations: 1000\n",
      "Best Lambda: 0.129714864167\n",
      "Square Loss on Test Data: 0.00787268083111\n",
      "Avg time to run <function vect_homotopy_lambda_search at 0x115ede230> after 1 trials: 5 seconds per trial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:11: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "def vect_homotopy_lambda_search():\n",
    "    t=0\n",
    "    Lambdas=[]\n",
    "    loss_hist=[]\n",
    "    loss_min = lambda_min=w_opt=np.nan\n",
    "    (N, D) = X.shape\n",
    "    Lambda_max = np.linalg.norm(np.dot(X.T,y),np.inf)\n",
    "    log_lambda_max = np.log10(Lambda_max)\n",
    "    print \"Start Searching\"\n",
    "    w_old = w = np.zeros(D)\n",
    "    for i in np.linspace(log_lambda_max, -2, 15):\n",
    "        Lambda = 10**i\n",
    "        print \"Lambda = {0}. Warm Starting.....\".format(Lambda)\n",
    "        \n",
    "        w_old = w\n",
    "        w, converged = vect_homotopy_lasso_shooting(X_train, y_train, Lambda=Lambda, w=w_old)\n",
    "\n",
    "        Lambdas.append(Lambda)\n",
    "        loss = compute_loss(X_validation, y_validation, w)\n",
    "        loss_hist.append(loss)\n",
    "        if t==0:\n",
    "            loss_min = loss\n",
    "            lambda_min = Lambda\n",
    "            w_min = w.copy()  \n",
    "        elif converged:\n",
    "            if loss<=loss_min:\n",
    "                loss_min = loss\n",
    "                lambda_min = Lambda\n",
    "                w_min = w.copy()  \n",
    "        t=t+1\n",
    "    best_lost = vect_homotopy_lasso_shooting(X_test, y_test, lambda_min)\n",
    "    print \"Best Lambda:\",lambda_min\n",
    "    print \"Square Loss on Test Data:\", loss_min\n",
    "    \n",
    "    return w_opt,Lambdas,loss_hist\n",
    "\n",
    "timeme(vect_homotopy_lambda_search)\n",
    "#%timeit w_lasso,lambdas_lasso,loss_hist_lasso = homotopy_lambda_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Deriving the Coordinate Minimizer for lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) First let's get a trival case out of the way. If $x_{ij}=0$ for $i=1,...,n$, what is the coordinate minimizer $w_j$?\n",
    "\n",
    "> __ANSWER__\n",
    "\n",
    "> $w_j=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Write the derivative of $f(w_j)$. \n",
    "\n",
    "> __ANSWER__\n",
    "\n",
    "> $\\begin{array}{cc} \n",
    "\\partial_{w_j}f(w_j) & = & (a_j w_j -c_j)+\\lambda \\partial_{w_j}\\|w_j\\|_1 \\\\ \n",
    "&=& a_j w_j-c_j+sign(w_j)\\lambda\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) If $w_j>0$ and minimizes $f$, then show that $w_j=-\\frac{1}{a_j}(\\lambda-c_j)$. Similarly, if $w_j<0$ and minimizes $f$, show that $w_j=\\frac{1}{a_j}(\\lambda+c_j)$. Give conditions on $c_j$ that imply the minimizer $w_j>0$ and $w_j<0$, respectively.\n",
    "\n",
    "> __ANSWER__\n",
    "\n",
    "> The magnitude of $c_j$ is an indication of relevance feature j is for predicting y.\n",
    "\n",
    "> If $c_j>\\lambda$, then feature $j$ is strongly possitively correlated with the residual, so $w_j>0$.\n",
    "\n",
    "> If $c_j<-\\lambda$, then feature $j$ is strongly negatively correlated with the residual, so $w_j<0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Derive expressions for the two one-sided derivative at $f(0)$, and show that $c_j \\in [-\\lambda, \\lambda]$ implies that $w_j=0$ is a minimizer.\n",
    "\n",
    "> __ANSWER__\n",
    "\n",
    "> $$a = \\lim_{w\\to0}\\frac{\\|w\\|-\\|0\\|}{w}=1$$\n",
    "\n",
    "> $$b = \\lim_{w\\to0}\\frac{\\|-w\\|)-\\|0\\|)}{w}=-1$$\n",
    "\n",
    "> The derivative for the $f(w_j)$ is \n",
    "\n",
    "> $$[-c_j+\\lambda b, -c_j +\\lambda a]=[-c_j-\\lambda,-c_j+\\lambda]$$\n",
    "\n",
    "> $\\therefore$ 0 is the minimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Conclude the minimizer.\n",
    "> If $c_j>\\lambda$, then feature $j$ is strongly possitively correlated with the residual, so $w_j>0$ and $w_j=-\\frac{1}{a_j}(\\lambda-c_j)$.\n",
    "\n",
    "> if $c_j\\in[-\\lambda,\\lambda]$, $w_j=0$ is a minimizer.\n",
    "\n",
    "> If $c_j<-\\lambda$, then feature $j$ is strongly negatively correlated with the residual, so $w_j<0$ and $w_j=\\frac{1}{a_j}(\\lambda+c_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Lasso Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Deriving $\\lambda_{max}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Compute $L_{'}(0;v)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $\\begin{array}{cc}\n",
    "L^{'}(0;v)&=&\\lim\\limits_{h\\to 0}\\frac{1}{h}[\\|Xhv-y\\|_2^2+\\lambda\\|hv\\|_1-\\|y\\|^2-\\lambda\\|0\\|]\\\\\n",
    "&=&-2(Xv)^T y+\\lambda\\|v\\|\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Since the Lasso objective is convex, for $w^*$ to be a minimizer of $L(w)$ we must have that the directional derivative $L^{'}(w^*;v)\\geq 0$ for all v. Starting form the condition $L^{'}(0;v)\\geq 0$, rearrange terms to get a lower bounds on $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $\\begin{array}{cc}\n",
    "0&\\leq&L^{'}(0;v)\\\\\n",
    "&\\leq&-2vX^T y+\\lambda\\|v\\|\\\\\n",
    "\\lambda&\\geq&\\frac{2v X^T y}{\\|v\\|_1}\\quad for\\quad every\\quad v\\\\\n",
    "\\end{array}$\n",
    "\n",
    "> $\\lambda$'s lower bound is $\\frac{2v X^T y}{\\|v\\|_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Since our lower bounds on $\\lambda$ for all $v$, we want to compute the maximum lower bound. Compute the maximum lower bound of $\\lambda$ by maximizing the expression over $v$. Show that this expression is equivalent to $\\lambda_{max}=2\\|X^Ty\\|_\\infty$.\n",
    "\n",
    "> __ANSWER__\n",
    "\n",
    "> $\\begin{array}{lll}\n",
    "L'(w,v) &=& \\lim\\limits_{h\\to 0}\\frac{L(w+hv)-L(w)}{h}\\\\\n",
    "&=&\\lim\\limits_{h\\to 0}\\frac{(X(w+hv)-y)^T(X(w+hv)-y)+\\lambda(\\|w+hv\\|_1-\\|w\\|_1)}{h}\\\\\n",
    "&=&2X^T(Xw-y)v+\\lambda(\\sum\\limits_{j,w_j\\neq0}sign(w_j)v_j+\\sum\\limits_{j,w_j=0}|v_j|)\n",
    "\\end{array}$\n",
    "\n",
    "> Lower bounds for all v and w should be,\n",
    "\n",
    "> $\\lambda_{max}\\geq\\frac{2X^T(y-Xw)v}{\\sum\\limits_{j,w_j\\neq0}sign(w_j)v_j+\\sum\\limits_{j,w_j=0}|v_j|}=2\\|X^Ty\\|_\\infty$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Show that for $L(w)=\\|Xw+b-y\\|^2_2+\\lambda\\|w\\|_1$,$\\lambda_{max}=2\\|X^T(y-\\bar y)\\|_\\infty$ where $\\bar y$ is the mean of valus in the vector $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $L(w) = \\sum\\limits_{i=1}^{n}(w^Tx_i+b-y_i)+\\lambda\\|w\\|_1$\n",
    "\n",
    "> $\\begin{array}{ll}\n",
    "0&\\leq&L^{'}(0;v)\\\\\n",
    "&\\leq&-2vX^T (y-b)+\\lambda\\|v\\|\\\\\n",
    "\\end{array}$\n",
    "\n",
    "> Therefore, \n",
    "\n",
    "> $\\begin{array}{ll}\n",
    "\\lambda &\\geq&2\\frac{v}{\\|v\\|_1} X^T (y-b)\\\\\n",
    "&=&2\\|X^T(y-b)\\|_\\infty\\\\\n",
    "&=&2\\|X^T(y-\\bar y+\\bar y-b)\\|\\\\\n",
    "&\\geq&2\\|X^T(y-\\bar y)\\|_\\infty\n",
    "\\end{array}$\n",
    "\n",
    "> Hence,\n",
    "\n",
    "> $\\lambda_{max} = 2\\|X^T(y-\\bar y)\\|_\\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Derive the relation between $\\hat \\theta_i$ and $\\hat \\theta_j$, the $i^{th}$ and the $j^{th}$ components of the optimal weight vector obtained by solving the Lasso optimization problem.\n",
    "\n",
    ">__ANSWER__\n",
    "\n",
    "> Assume $\\hat{\\theta}_i=a$ and $\\hat{\\theta}_j=b$\n",
    "\n",
    ">The lasso objective function, below, must minimize both loss and regularization.\n",
    "\n",
    ">$$\\sum\\limits_{k=1}^n (h(x_k) - y_k)^2 + \\lambda\\|w\\|_1$$\n",
    "\n",
    ">First consider the loss, $\\sum\\limits_{k=1}^n (h(x_k) - y_k)^2$, where $h(x_k)=\\mathbf w^Tx_k$\n",
    "\n",
    ">The loss due to $x_i$, $x_j$ is $\\sum\\limits_{k=1}^n (\\hat{\\theta}_iX_{ik} + \\hat{\\theta}_jX_{jk} - y_k)$\n",
    "\n",
    "> Since $X_i=X_j$, this simplifies to $\\sum\\limits_{k=1}^n ((\\hat{\\theta}_i + \\hat{\\theta}_j)X_{ik} - y_k)$\n",
    "\n",
    "> Thus the optimal values $a$ and $b$ must sum to another value $c$ that minimizes this expression $\\sum\\limits_{k=1}^n (cX_{ik} - y_k)$\n",
    "\n",
    "> Next we minimize the lasso regularization component, $\\lambda||w||_1 = \\lambda\\sum\\limits_{l=1}^d|w_l|$\n",
    "\n",
    "> The regularization penalty due to $x_i$, $x_j$ is $|a| + |b|$.  If $a$ and $b$ are of opposite sign, and both are nonzero, then $|a| + |b|$ > $|c| + |0|$, and the regularization penalty is not minimized.  Therefore $a$ and $b$ must be of the same sign and are constrained by optimal value $c$ such that $a+b=c$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Derive the relation between $\\hat{\\theta}_i$ and $\\hat{\\theta}_j$, the $i^{th}$ and $j^{th}$ components of the optimal weight vector obtained by solving the ridge regression optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The ridge regression objective function, below, must minimize both loss and regularization.\n",
    "\n",
    ">$\\sum\\limits_{k=1}^n (h(x_k) - y_k)^2 + \\lambda\\|w\\|_2^2$\n",
    "\n",
    ">The loss component is the same as with lasso, so we must minimize the regularization penalty subject to the same constraint as in lasso, which is that $a + b = c$\n",
    "\n",
    ">The regularization penalty due to $x_i$, $x_j$ is $a^2 + b^2$.  Next I will show that $a^2 + b^2 \\ge (\\frac{c}{2})^2 + (\\frac{c}{2})^2$, and therefore $a$ and $b$ must be equal to $\\frac{c}{2}=\\frac{a+b}{2}$ under these conditions.\n",
    "\n",
    "> Claim: $a^2 + b^2 \\ge 2(\\frac{c}{2})^2$\n",
    "\n",
    "> Proof: \n",
    "\n",
    ">$\\begin{array}{lll}\n",
    "a^2 + b^2 &=& a^2 + (c-a)^2\\\\\n",
    "&=& a^2 + c^2 - 2ac + a^2\\\\\n",
    "&=& 2a^2 + c^2 - 2ac\\\\\n",
    "&=& \\frac{1}{2}(4a^2 - 4ac + 2c^2)\\\\\n",
    "&=& \\frac{1}{2}(2a-c)^2 + \\frac{c^2}{2}\\\\\n",
    "&=& \\frac{1}{2}(2a-c)^2 + 2(\\frac{c}{2})^2\\\\\n",
    "&=& \\frac{1}{2}(2a-c)^2 \\ge 0\n",
    "\\end{array}$\n",
    "\n",
    "> therefore $\\frac{1}{2}(2a-c)^2 + 2(\\frac{c}{2})^2 \\ge 2(\\frac{c}{2})^2$\n",
    "\n",
    "> Thus $a$ and $b$ must be equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 The Ellipsoids in the $\\ell_1/\\ell_2$ regularization picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Let $\\hat w = (X^T X)^{-1}X^Ty$. Show that $\\hat w$ has empirical risk given by\n",
    "$$\\hat R_n(\\hat w)=\\frac{1}{n}(-y^TX\\hat w+y^Ty)$$\n",
    "\n",
    "> __ANSWER__\n",
    "\n",
    "> $\\begin{array}{lll}\n",
    "\\hat R_n(\\hat w) &=& \\frac{1}{n}(Xw-y)^T(Xw-y) \\\\\n",
    "&=& \\frac{1}{n}(w^TX^TXw-2w^TX^Ty+y^Ty)\\\\\n",
    "&=& \\frac{1}{n}[w^TX^T(Xw-2y)+y^Ty]\\\\\n",
    "&=& \\frac{1}{n}[w^TX^T(-2y+X(X^T X)^{-1}X^Ty)+y^Ty]\\\\\n",
    "&=& \\frac{1}{n}[-w^TX^Ty+y^Ty]\\\\\n",
    "&=& \\frac{1}{n}\\{-[(X^T X)^{-1}X^Ty]^TX^Ty+y^Ty\\}\\\\\n",
    "&=& \\frac{1}{n}[-y^TX[(X^TX)^{-1}X^Ty]+y^Ty]\\\\\n",
    "&=& \\frac{1}{n}[-y^TX\\hat w+y^Ty]\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Show that for any $w$ we have\n",
    "$$\\hat R_n(w)=\\frac{1}{n}(w-\\hat w)^TX^TX(w-\\hat w)+\\hat R_n(\\hat w)$$\n",
    "> __ANSWER__\n",
    "\n",
    "> $\\begin{array}{lll}\n",
    "\\hat R_n(w) &=& \\frac{1}{n}(Xw-y)^T(Xw-y)\\\\\n",
    "&=& \\frac{1}{n}[w^T(X^TX)w-2(X^Ty)^Tw+y^Ty]\\\\\n",
    "&=& \\frac{1}{n}\\{[w-(X^TX)^{-1}X^Ty]^T(X^TX)[w-(X^TX)^{-1}X^Ty]-(X^Ty)^T(X^TX)^{-1}X^Ty+y^Ty\\}\\\\\n",
    "&=& \\frac{1}{n}[(w-\\hat w)^T(X^TX)(w-\\hat w)-y^TX\\hat w+y^Ty]\\\\\n",
    "&=& \\frac{1}{n}(w-\\hat w)^T(X^TX)(w-\\hat w)+\\hat R_n(\\hat w)\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Using the expression in (2), give a very short proof that $\\hat w=(X^TX)^{-1}X^Ty$ is the empirical risk minimizer. That is:\n",
    "$$\\hat w=\\underset{w}{\\mathrm{argmin}}\\hat R_n(w)$$\n",
    "\n",
    "> __ANSWER__\n",
    "\n",
    "> $X^TX$ is positive semidefinite\n",
    "\n",
    "> $\\therefore \\forall (w-\\hat w)\\in \\mathbf R^d, $\n",
    "\n",
    "> $\\Phi(w)=(w-\\hat w)^TX^TX(w-\\hat w)\\geq 0$\n",
    "\n",
    "> Hence $\\hat w$ is the minimizer of $\\Phi(w)$\n",
    "\n",
    "> $\\hat R_n(w)=\\frac{1}{n}\\Phi(w)+\\hat R_n(\\hat w)\\geq \\hat R_n(\\hat w)$\n",
    "\n",
    "> Therefore, $\\hat w$ is also the minimizer fo the empirical risk $\\hat R_n(w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Give an expression for the set of $w$ for which the empirical risk exceeds the minimum empirical risk $\\hat R_n(\\hat w)$ by an amount $c>0$. This set is an ellipse - what is its center?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $\\{w\\quad |\\quad (w-\\hat w)^TX^TX(w-\\hat w)=c,c>0\\}$ is an ellipsoid centered at $\\hat w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Projected SGD via Variable Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Implement projected SGD to solve the above optimization problem for the same value $\\lambda$'s as used with the shooting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sgd_loss(X, y, w_p, w_n, Lambda):\n",
    "    m = y.size\n",
    "    hX = (w_p - w_n).dot(X.T)\n",
    "    #loss = ((np.square(hX - y))/(m)).sum() + Lambda*((w_p+w_n).sum())\n",
    "    loss = ((np.square(hX - y))/(2*m)).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_sgd_gradient(X, y, i, w_p, w_n, Lambda, component):\n",
    "    if component is 'p':\n",
    "        grad = (X[i].dot(w_p-w_n)-y[i])*(X[i]) + Lambda\n",
    "    elif component is 'n':\n",
    "        grad = -(X[i].dot(w_p-w_n)-y[i])*(X[i]) + Lambda\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def projected_sgd(X,y,validation_X, validation_y, Lambda=0.3, alpha=0.1, beta=0.1, num_iter=5000):\n",
    "    m,d = X.shape\n",
    "    w = np.random.rand(d,1) #initialize weight\n",
    "    w_p = np.zeros(d)\n",
    "    w_n = np.zeros(d)\n",
    "    for i in range(d):\n",
    "        if w[i] > 0:\n",
    "            w_p[i] = w[i]\n",
    "        else:\n",
    "            w_n[i] = w[i]\n",
    "    \n",
    "    opt_loss=1000\n",
    "    opt_w = w\n",
    "    step=0\n",
    "    epoch=0\n",
    "    \n",
    "    order = range(m)\n",
    "    \n",
    "    for j in range(num_iter + 1):\n",
    "        np.random.shuffle(order)\n",
    "        \n",
    "        for k in range(m):\n",
    "            loss = compute_sgd_loss(validation_X, validation_y, w_p, w_n, Lambda)\n",
    "            grad_p = compute_sgd_gradient(X, y, order[k], w_p, w_n, Lambda, component='p')\n",
    "            grad_n = compute_sgd_gradient(X, y, order[k], w_p, w_n, Lambda, component='n')\n",
    "            \n",
    "            if loss < opt_loss:\n",
    "                opt_loss = loss\n",
    "                opt_w = w_p - w_n\n",
    "                step = j*m+k+1\n",
    "                epoch= j+1\n",
    "            \n",
    "            if alpha is \"1/t\":\n",
    "                alpha=beta*(1./(j*m+k+1))\n",
    "            elif alpha is \"1/sqrt(t)\":\n",
    "                alpha=beta*(1./np.sqrt(j*m+k+1))\n",
    "            else:\n",
    "                alpha=alpha\n",
    "            \n",
    "            w_p = w_p - alpha*grad_p\n",
    "            w_n = w_n - alpha*grad_n\n",
    "            \n",
    "            for i in range(d):\n",
    "                if w_p[i] < 0:\n",
    "                    w_p[i] = 0\n",
    "                elif w_n[i] < 0:\n",
    "                    w_n[i] = 0\n",
    "    return opt_loss, opt_w, w_p, w_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Searching\n",
      "Converged: False Iterations: 1000\n",
      "Converged: False Iterations: 1000\n",
      "Converged: False Iterations: 1000\n",
      "Converged: False Iterations: 1000\n",
      "Converged: False Iterations: 1000\n",
      "Converged: False Iterations: 1000\n",
      "Converged: True Iterations: 587\n",
      "Converged: True Iterations: 310\n",
      "Converged: True Iterations: 213\n",
      "Converged: True Iterations: 178\n",
      "Converged: True Iterations: 157\n",
      "Converged: True Iterations: 129\n",
      "Converged: True Iterations: 52\n",
      "Converged: True Iterations: 47\n",
      "Converged: True Iterations: 22\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "w = []\n",
    "loss1=[]\n",
    "w1=[]\n",
    "Lambda_max = np.linalg.norm(np.dot(X_train.T,y_train),np.inf)\n",
    "log_lambda_max = np.log10(Lambda_max)\n",
    "print \"Start Searching\"\n",
    "\n",
    "for i in np.linspace( -2,log_lambda_max, 15):\n",
    "    Lambda = 10**i;\n",
    "    opt_loss, opt_w, w_p, w_n = projected_sgd(X_train,y_train,X_validation, y_validation, Lambda=Lambda, alpha=0.005, num_iter=5000)\n",
    "    opt_w1, converged = lasso_shooting(X_train, y_train, Lambda)    \n",
    "    opt_loss1 = compute_loss(X_validation, y_validation, opt_w1)\n",
    "    loss.append(opt_loss)\n",
    "    w.append(opt_w)\n",
    "    w1.append(opt_w1)\n",
    "    loss1.append(opt_loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x115dc8910>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEdCAYAAAD0NOuvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115ed6490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([10**i for i in np.linspace( -2,log_lambda_max, 15)], loss, label=\"Projected SGD\")\n",
    "plt.plot([10**i for i in np.linspace( -2,log_lambda_max, 15)], loss1, label=\"Shooting algo\")\n",
    "plt.gca().set_xscale(\"log\")\n",
    "plt.gca().set_yscale(\"log\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Comparing Projected SGD & Shooting algo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With tolerence of 0.05\n",
      "True Value Zero estimated as Nonzero: 0\n",
      "True Value Nonzero estimated as Zero: 0\n",
      "True Value Zero estimated as Zero: 65\n",
      "True Value Nonzero estimated as Nonzero: 10\n"
     ]
    }
   ],
   "source": [
    "#Analyze the sparsity of the projected SGD solution\n",
    "loss_array = np.array(loss)\n",
    "indx = np.where(loss_array ==loss_array.min())[0][0]\n",
    "w_opt = w[indx]\n",
    "### Coefficient Report under tolerence=0.05 ###\n",
    "tolerance = 0.05\n",
    "false_nonzeros = sum((w_true==0) & (abs(w_opt)>=tolerance))\n",
    "false_zeros = sum((w_true!=0) & (abs(w_opt)<tolerance))\n",
    "true_nonzeros = sum((w_true!=0) & (abs(w_opt)>=tolerance))\n",
    "true_zeros = sum((w_true==0) & (abs(w_opt)<tolerance))\n",
    "print \"With tolerence of {0}\".format(tolerance)\n",
    "print \"True Value Zero estimated as Nonzero: {0}\".format(false_nonzeros)\n",
    "print \"True Value Nonzero estimated as Zero: {0}\".format(false_zeros)\n",
    "print \"True Value Zero estimated as Zero: {0}\".format(true_zeros)\n",
    "print \"True Value Nonzero estimated as Nonzero: {0}\".format(true_nonzeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With tolerence of 0.05\n",
      "True Value Zero estimated as Nonzero: 5\n",
      "True Value Nonzero estimated as Zero: 0\n",
      "True Value Zero estimated as Zero: 60\n",
      "True Value Nonzero estimated as Nonzero: 10\n"
     ]
    }
   ],
   "source": [
    "#Analyze the sparsity of the projected SGD solution\n",
    "loss_array = np.array(loss1)\n",
    "indx = np.where(loss_array ==loss_array.min())[0][0]\n",
    "w_opt = w1[indx]\n",
    "### Coefficient Report under tolerence=0.05 ###\n",
    "tolerance = 0.05\n",
    "false_nonzeros = sum((w_true==0) & (abs(w_opt)>=tolerance))\n",
    "false_zeros = sum((w_true!=0) & (abs(w_opt)<tolerance))\n",
    "true_nonzeros = sum((w_true!=0) & (abs(w_opt)>=tolerance))\n",
    "true_zeros = sum((w_true==0) & (abs(w_opt)<tolerance))\n",
    "print \"With tolerence of {0}\".format(tolerance)\n",
    "print \"True Value Zero estimated as Nonzero: {0}\".format(false_nonzeros)\n",
    "print \"True Value Nonzero estimated as Zero: {0}\".format(false_zeros)\n",
    "print \"True Value Zero estimated as Zero: {0}\".format(true_zeros)\n",
    "print \"True Value Nonzero estimated as Nonzero: {0}\".format(true_nonzeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Projected SGD with Lasso Shooting, the Projected SGD solution has more sparsity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
