{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Coin-Flipping:-Maximum-Likelihood-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Coin Flipping: Maximum Likelihood</a></div><div class=\"lev2\"><a href=\"#Question-1-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Question 1</a></div><div class=\"lev2\"><a href=\"#Question-2-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Question 2</a></div><div class=\"lev2\"><a href=\"#Question-3-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Question 3</a></div><div class=\"lev2\"><a href=\"#Question-4-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Question 4</a></div><div class=\"lev1\"><a href=\"#Coin-Flipping:-Bayesian-Approach-with-Beta-Prior-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Coin Flipping: Bayesian Approach with Beta Prior</a></div><div class=\"lev2\"><a href=\"#Question-1-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Question 1</a></div><div class=\"lev2\"><a href=\"#Question-2-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Question 2</a></div><div class=\"lev2\"><a href=\"#Question-3-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Question 3</a></div><div class=\"lev2\"><a href=\"#Question-4-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Question 4</a></div><div class=\"lev2\"><a href=\"#Question-5-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Question 5</a></div><div class=\"lev1\"><a href=\"#Hierarchical-Bayes-for-Click-Through-Rate-Estimation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Hierarchical Bayes for Click-Through Rate Estimation</a></div><div class=\"lev2\"><a href=\"#Empirical-Bayes-for-a-single-app-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Empirical Bayes for a single app</a></div><div class=\"lev2\"><a href=\"#Empirical-Bayes-Using-All-App-Data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Empirical Bayes Using All App Data</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DS-GA 1003: Machine Learning and Computational Statistics**\n",
    "\n",
    "Homework 7: Bayesian Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin Flipping: Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Suppose we flip a coin and get the following seqeunce of heads and tails: $$\\mathcal D = (H, H, T)$$  Give an expression for the probability of observing $\\mathcal D$ given that the probability of heads is $\\theta$. That is, give an expression for $p(D | \\theta)$. **This is called the likelihood of $\\theta$ for the data $D$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $p(D|\\theta)=\\theta*\\theta*(1-\\theta)$\n",
    "\n",
    "> $= \\theta^2-\\theta^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "How many different sequences of 3 coin tosses have 2 heads and 1 tail? If we coss the coin 3 times, what is the probability of 2 heads and 1 tail? (Answer should be in terms of $\\theta$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> There are three different sequences of 2 heads and 1 tail. {HHT, HTH,THH}\n",
    "\n",
    "> The probability to get 2 heads and 1 tail is $3\\theta^2-3\\theta^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "More generally, give an expression for the likelihood $p(D|\\theta)$ for a particular sequence of flips D that has $n_h$ heads and $n_t$ tails. Make sure you have expressions that make sense even for $\\theta=0$ and $n_h=0$, and other boundary cases. You may use the convention that $0^0 = 1$, or you can break your expression into cases if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $p(D|\\theta)=\\theta^{n_h}(1-\\theta)^{n_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Prove that the maximum likelihood estimate of $\\theta$ given we observed a sequence with $n_h$ heads and $n_t$ tails is\n",
    "$$ \\hat θ_{MLE} =\\frac{n_h}{n_h+n_t}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  __ANSWER__\n",
    "\n",
    "> $log(p(D|\\theta)) = n_hlog\\theta+n_tlog(1-\\theta)$\n",
    "\n",
    "> for which we take the derivative, \n",
    "\n",
    "> $\\frac{\\partial log(p(D|\\theta))}{\\partial \\theta}=\\frac{n_h}{\\theta}-\\frac{n_t}{1-\\theta}=0$\n",
    "\n",
    "> $\\theta = \\frac{n_h}{n_h+n_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin Flipping: Bayesian Approach with Beta Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll now take a Bayesian approach to the coin flipping problem, in which we treat $\\theta$ as a random variable sampled from some prior distribution $p(\\theta)$. We’ll represent the ith coin flip by a random variable $X_i\\in{0,1}$, where $X_i=1$ if the $i$th flip is heads. We assume that the $X_i’s$ are conditionally indendent given $\\theta$. This means that the joint distribution of the coin flips and $\\theta$ factorizes as follows:\n",
    "$$p(x_1,\\cdots, x_n,\\theta) = p(\\theta)p(x_1,\\cdots, x_n|\\theta) (always true)$$\n",
    "$$= p(\\theta)\\Pi_{i=1}^n p(x_i| θ) (by conditional independence).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Suppose that our prior distribution on $\\theta$ is $Beta(h, t)$, for some $h,t>0$. That is, $p(\\theta)\\propto\\theta^{h−1}(1-\\theta)^{t−1}$. Suppose that our sequence of flips D has $n_h$ heads and $n_t$ tails. Show that the posterior distribution for $\\theta$ is $Beta(h+n_h,t+n_t)$. That is, show that $$p(\\theta|D)\\propto\\theta^{h−1+n_h}(1-\\theta)^{t−1+n_t}.$$\n",
    "We say that the Beta distribution is **conjugate** to the Bernoulli distribution since the prior\n",
    "and the posterior are both in the same family of distributions (i.e. both Beta distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $p(\\theta|D)\\propto p(\\theta)p(D|\\theta)$\n",
    "\n",
    "> $=\\theta^{h-1}(1-\\theta)^{t-1}\\times\\theta^{n_h}(1-\\theta)^{n_t}$\n",
    "\n",
    "> $=\\theta^{h-1+n_h}(1-\\theta)^{t-1+n_t}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Give expressions for the MLE, the MAP, and the posterior mean estimates of $\\theta$. [Hint: You may use the fact that a Beta(h, t) distribution has mean h/(h + t) and has mode (h − 1) / (h + t − 2) for h, t > 1. For the Bayesian solutions, you should note that as h + t gets very large, the posterior mean and MAP approach the prior mean h/ (n + h), while for fixed h and t, the posterior mean approaches the MLE when $n_h+n_t\\to\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $\\hat\\theta_{MLE}=argmin_{\\theta}log(p(\\theta|D))=\\frac{h-1+n_h}{h-1+n_h+t-1+n_t}$\n",
    "\n",
    "> $\\hat\\theta_{MAP}=argmax_{\\theta}p(\\theta|D)=\\frac{h-1}{h+t-2}$\n",
    "\n",
    "> $\\hat\\theta_{POSTERIOR}=E(\\theta|D)=\\frac{h+n_h}{h+t+n_h+n_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "What happens to $\\hat\\theta_{MLE}$,$\\hat\\theta_{MAP}$, and $\\hat\\theta_{POSTERIOR MEAN}$ as the number of coin flips approaches infinity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> As the number of coin flips approaches infinity, $\\hat\\theta_{MAP}$ will still remain $\\frac{h-1}{h+t-2}$, while $\\hat\\theta_{MLE}$ and $\\hat\\theta_{POSTERIORMEAN}$ will become $\\frac{n_h}{n_h+n_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "The MAP and posterior mean estimators of $\\theta$ were derived from a Bayesian perspective. Let’s now evaluate them from a frequentist perspective. Suppose $\\theta$ is fixed and unknown. Which of the MLE, MAP, and posterior mean estimators give unbiased estimates of θ, if any? [Hint:The answer may depend on the parameters h and t of the prior.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $E(\\theta_{MAP})=E(argmax_{\\theta}p(\\theta|D))=max_{\\theta}(p(\\theta|D))$\n",
    "\n",
    "> $E(\\theta_{MLE})=argmin_{\\theta}p(\\theta|D)=min_{\\theta}(p(\\theta|D))$\n",
    "\n",
    "> $E(\\theta_{POSETERIOR})=\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Suppose somebody gives you a coin and asks you to give an estimate of the probability of heads, but you can only toss the coin 3 times. You have no particular reason to believe this is an unfair coin. Would you prefer the MLE or the posterior mean as a point estimate of $\\theta$? If the posterior mean, what would you use for your prior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> I would use posterior mean, and the prior I will use will be the beta distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Bayes for Click-Through Rate Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Bayes for a single app\n",
    "We start by working out some details for Bayesian inference for a single app. That is, suppose we only have the data $D_i$ from app $i$, and nothing else. Mathematically, this is exactly the same setting as the coin tossing setting above, but here we push it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Give an expression for $p(D_i|\\theta_i)$, the likelihood of $D_i$ given the probability of click $\\theta_i$, in terms of $\\theta_i$, $x_i$ and $n_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $p(D_i|\\theta_i)=\\theta_i^{x_i}(1-\\theta_i)^{n_i-x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 The probability density for the $Beta(a,b)$ distribution, evaluated at $\\theta_i$, is given by \n",
    "$$Beta(\\theta_i;a,b)=\\frac{1}{B(a,b)}\\theta_i^{\\alpha-1}(1-\\theta_i)^{b−1}$$\n",
    "\n",
    "where $B(a,b)$ is called the Beta function. Explain why we must have $\\int\\theta^{a−1}_i(1-\\theta_i)^{b−1}d\\theta=B(a, b)$, and give the full density function for the prior on $\\theta_i$, in terms of $a,b$, and the normalization function B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> We need the total probability integrals to 1, so we must have $\\int\\theta^{a−1}_i(1-\\theta_i)^{b−1}d\\theta=B(a, b)$\n",
    "\n",
    "> The full density function for $\\theta_i$ is \n",
    "$$Beta(\\theta_i;a,b)=\\frac{1}{B(a,b)}\\theta_i^{\\alpha-1}(1-\\theta_i)^{b−1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Give an expression for the posterior distribution $p(\\theta_i|D_i)$. In this case, include the constant of proportionality. (In other words, do not use the “is proportional to” sign $\\propto$ in your final expression.) [Hint: This problem is essentially a repetition of an earlier problem.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $p(\\theta_i|D_i)=c p(\\theta_i)p(D_i|\\theta_i)$\n",
    "\n",
    "> $=\\frac{c}{B(a,b)}\\theta_i^{a-1+x_i}(1-\\theta_i)^{b-1+n_i-x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Give a closed form expression for $p(D_i)$, the marginal likelihood of $D_i$, in terms of the $a$, $b$, $x_i$, and $n_i$. You may use the normalization function $B(·, ·)$ for convenience, but you should not have any integrals in your solution. (Hint: $p(D_i) = p (D_i|\\theta_i)p(\\theta_i)d\\theta_i$, and the answer will be a ratio of two beta function evaluations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $p(D_i)=\\int p(D_i|\\theta_i)p(\\theta_i)d\\theta(i)$\n",
    "\n",
    "> $=\\frac{1}{B(a,b)}\\theta_i^{a-1}(1-\\theta_i)^{b-1}\\theta^{x_i}(1-\\theta)^{n_i-x_i}$\n",
    "\n",
    "> $=\\frac{1}{B(a,b)}\\theta_i^{a-1+x_i}(1-\\theta_i)^{b-1+n_i-x_i}$\n",
    "\n",
    "> $=\\frac{B(a-1+x_i, b-1+n_i-x_i)}{B(a,b)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 The maximum likelihood estimate for $\\theta_i$ is $x_i/n_i$. Let $p_{MLE}(D_i)$ be the marginal likelihood\n",
    "of $D_i$ when we use a prior on $\\theta_i$ that puts all of its probability mass at $x_i/n_i$d. Note that\n",
    "$p_{MLE}(D_i) = p(D_i|\\theta_i =\\frac{x_i}{n_i}p(\\theta_i =\\frac{x_i}{n_i}= p(D_i|\\theta_i) =\\frac{x_i}{n_i}$.\n",
    "Explain why, or prove, that $p_{MLE}(D_i)$ is larger than $p(D_i)$ for any other prior we might put on $\\theta_i$. If it’s too hard to reason about all possible priors, it’s fine to just consider all Beta priors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $p(D_i)=\\int p(D_i|\\theta_i)p(\\theta_i)d\\theta_i$\n",
    "\n",
    "> $\\approx \\sum p(D_i|\\theta_i)p(\\theta_i)$\n",
    "\n",
    "> $<\\sum p(D_i|\\theta_i=\\frac{x_i}{n_i})p(\\theta_i)$\n",
    "\n",
    "> $<p_{MLE}D_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 One approach to getting an empirical Bayes estimate of the parameters a and b is to use maximum likelihood. Such an empirical Bayes estimate is often called an ML-2 estimate, since it’s maximum likelihood, but at a higher level in the Bayesian hierarchy. To emphasize the dependence of the likelihood of $D_i$ on the parameters a and b, we’ll now write it as $p(D_i| a, b)$. The empirical Bayes estimates for a and b are given by\n",
    "\n",
    "$$(\\hat a, \\hat b) = arg\\max\\limits_{(a,b)∈R>0×R>0} p(D_i | a, b).$$\n",
    "\n",
    "To make things concrete, suppose we observed $x_i = 3$ clicks out of $n_i = 500$ impressions. A plot of $p(D_i| a, b)$ as a function of a and b is given in Figure 1. It appears from this plot that the likelihood will keep increasing as a and b increase, at least if a and b maintain a particular ratio. Indeed, this likelihood function never attains its maximum, so we cannot use ML-2 here. Explain what’s happening to the prior as we continue to increase the likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Bayes Using All App Data\n",
    "\n",
    "In the previous section, we considered working with data from a single app. With a fixed prior, such as Beta(3,400), our Bayesian estimates for $\\theta_i$ seem more reasonable (to me, the person who chose the prior) than the MLE when our sample size $n_i$ is small. The fact that these estimates seem reasonable is an immediate consequence of the fact that I chose the prior to give high probability to estimates that seem reasonable to me, before ever seeing the data. Our attempt to use empirical Bayes (ML-2) to choose the prior in a data-driven way was not successful. With only a single app, we were essentially overfitting the prior to the data we have. In this section, we’ll consider using the data from all the apps, in which case empirical Bayes makes more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Let $D=(D_1, \\cdots , D_d)$ be the data from all the apps. Give an expression for $p(D | a, b)$, the marginal likelihood of D. Expression should be in terms of $a, b, x_i, n_i$ for $i = 1, \\cdots, d$. (Hint: This problem should be easy, based on a problem from the previous section.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $p(D|a,b)=\\frac{B(a-1+x_i, b-1+n_i-x_i)}{B(a,b)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Explain why $p(\\theta_i| D) = p(\\theta_i| Di)$, according to our model. In other words, once we choose values for parameters a and b, information about one app does not give any information about other apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "toc_cell": true,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
