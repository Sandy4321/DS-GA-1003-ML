{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Computational Statistics, Spring 2016\n",
    "Homework 4: Kernels, Duals, and Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 Positive Semidefinite Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 _Give an example of an orthogonal matrix that is not symmetric._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> Let M = $\\begin{pmatrix}\n",
    "-\\frac{\\sqrt 2}{2} & \\frac{\\sqrt 2}{2}\\\\\n",
    "-\\frac{\\sqrt 2}{2} & -\\frac{\\sqrt 2}{2}\\\\\n",
    "\\end{pmatrix}$, and $M^T = $ $\\begin{pmatrix}\n",
    "-\\frac{\\sqrt 2}{2} & -\\frac{\\sqrt 2}{2}\\\\\n",
    "\\frac{\\sqrt 2}{2} & -\\frac{\\sqrt 2}{2}\\\\\n",
    "\\end{pmatrix}$, so M is not symmetric.\n",
    "\n",
    "> but $M^TM = \\begin {pmatrix} 1 & 0\\\\ 0 & 1 \\end{pmatrix}$, so M is orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 _Use the defination of a psd matrix and the spectral therom to show that all eigenvalues of a positive semidefinite matrix M are non-negative._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> If a real, orthogonal metrix M is psd, then for any $x\\in \\mathbf{R}^n$, $x^TMx\\geq 0$.\n",
    "\n",
    "> According to the spectral theorem, $M \\in \\mathbf R^{n\\times n}$ can be diagonalized as $M=Q\\Sigma Q^T$,\n",
    "\n",
    "> $\\Sigma = Q^TMQ$\n",
    "\n",
    "> Note that Q is the matrix of eigenvectors of M, that is,\n",
    "$$Q =\\begin{pmatrix}| &  & |\\\\\n",
    "q_{1} & \\cdots & q_{n}\\\\\n",
    "| &  & |\n",
    "\\end{pmatrix}$$,\n",
    "\n",
    "> $Q^TMQ = \n",
    "\\begin{pmatrix}\n",
    "q_1^TMq_1 & q_1^TMq_2 & \\cdots & q_1^TMq_n\\\\\n",
    "q_2^TMq_1 & q_2^TMq_2 & \\cdots & q_2^TMq_n\\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots\\\\\n",
    "q_n^TMq_1 & q_n^TMq_2 & \\cdots & q_n^TMq_n\\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "> The eigenvalue of psd matrix M is $diag(Q^TMQ) = (q_1^TMq_1,q_2^TMq_2,\\cdots,q_n^TMq_n)$,\n",
    "\n",
    "> So the eigenvalues of M are all non-negative as $\\forall q_1,...,q_n, q_i\\in R_n.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 In this problem we show that a psd matrix is a matrix version of a non-negative scalar, in that they both have a \"square root\". Show that a symmetric matrix M can be expressed as $BB^T$ for some matrix B, if and only if M is psd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> If $M=BB^T$, $\\forall x \\in \\mathbf R^n,$\n",
    "\n",
    ">> $x^TMx$\n",
    "\n",
    ">> $=x^TBB^Tx$\n",
    "\n",
    ">> $=(B^Tx)^TB^Tx$\n",
    "\n",
    ">> $\\geq 0,$\n",
    "\n",
    ">> so the symmetrix matrix M is psd;\n",
    "\n",
    "> Elsewhile if M is psd,\n",
    "\n",
    ">> Use the Spectral Theorem,\n",
    "\n",
    ">> $M = Q\\Sigma Q^T$\n",
    "\n",
    ">> Note that $\\Sigma$ is the matrix of M's eigenvalues, and all the eigenvalues are non-negative, we can get,\n",
    "\n",
    ">> $\\Sigma = {\\Sigma^{\\frac{1}{2}}}^T \\Sigma^{\\frac{1}{2}}$\n",
    "\n",
    ">> $M=Q{\\Sigma^{\\frac{1}{2}}}^T \\Sigma^{\\frac{1}{2}}Q^T$\n",
    "\n",
    ">> $={(\\Sigma^{\\frac{1}{2}}Q^T})^T\\Sigma^{\\frac{1}{2}}Q^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Positive Definite Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 _Show that all eigenvalues of a symmetric positive definite matrix are positive._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> Use the Spectral Theorem, for the real, symmetric matrix M, we have\n",
    "\n",
    "> $\\Sigma = Q^TMQ$, where $Q$ is the matrix of the M's eigenvectors,\n",
    "\n",
    "> $Q = \\begin{pmatrix}\n",
    "| & & | \\\\\n",
    "q_1 & \\cdots & q_n \\\\\n",
    "| & & | \\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "> $Q^TMQ=\\begin{pmatrix}\n",
    "q_1^TMq_1 & q_1^TMq_2 & \\cdots & q_1^TMq_n\\\\\n",
    "q_2^TMq_1 & q_2^TMq_2 & \\cdots & q_2^TMq_n\\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "q_n^TMq_1 & q_n^TMq_2 & \\cdots & q_n^TMq_n\\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "> $=\\begin{pmatrix}\n",
    "q_1^TMq_1 & 0 & \\cdots & 0\\\\\n",
    "0 & q_2^TMq_2 & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "0 & 02 & \\cdots & q_n^TMq_n\\\\\n",
    "\\end{pmatrix} $\n",
    "\n",
    "> So if M is positive definite, for all i=1,...,n $$q_i^TMq_i>0,$$\n",
    "\n",
    "> Which means all the eigenvalues will be greater than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 _Let M be a symmetric positive definite matrix. By the spectral theorem, $M=Q\\Sigma Q^T$, where $\\Sigma$ is a diagonal matrix of the eigenvalues of M. By the previous problem, all diagonal entries of $\\Sigma$ are positive. If $\\Sigma=diag(\\sigma_1,...,\\sigma_n)$, then $\\Sigma^{-1}=diag(\\sigma_1^{-1},...,\\sigma_n^{-1})$. Show that the matrix $Q\\Sigma^{-1}Q^T$is the inverse of M._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> Let $\\hat M=Q\\Sigma^{-1}Q^T$, \n",
    "\n",
    "> $\\hat MM=Q\\Sigma Q^TQ\\Sigma^{-1}Q^T$\n",
    "\n",
    "> = $Q\\Sigma \\Sigma^{-1}Q^T$\n",
    "\n",
    "> = $QQ^T$\n",
    "\n",
    "> = 1\n",
    "\n",
    "> So $M=Q\\Sigma^{-1}Q^T$ is the inverse of M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.3 Since positive semidefinite matrices may have eigenvalues that are zero, we see by the previous\n",
    "problem that not all psd matrices are invertible. Show that if M is a psd matrix and I is\n",
    "the identity matrix, then $M+\\lambda I$ is symmetric positive definite for any $\\lambda>0$, and give an\n",
    "expression for the inverse of $M+\\lambda I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For $\\forall x\\in \\mathbf{R}^{n}$ with $x\\ne0$,\n",
    "\n",
    "> $x^T(M+\\lambda I)x=x^TMx+\\lambda x^Tx$\n",
    "\n",
    "> Since M is psd,\n",
    "\n",
    "> $x^TMx\\geq0$ and $x^x>0$,\n",
    "\n",
    "> $x^T(M+\\lambda I)x>0$,\n",
    "\n",
    "> Therefore $M+\\lambda I$ is symmetric positive definite.\n",
    "\n",
    "> $M+\\lambda I = Q(\\Sigma+\\lambda I)Q^T$,\n",
    "\n",
    "> So $Q(\\Sigma+\\lambda I)^{-1}Q^T$ will be the inverse of $M+\\lambda I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Let M and N be symmetric matrices, with M positive semidefinite and N positive definite. Use the definitions of psd and spd to show that M + N is symmetric positive definite. Thus M + N is invertible. (Hint: For any $x\\ne 0$, show that $x^T(M + N)x > 0$. Also note that $x^T(M + N)x = x^TMx + x^T Nx$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> For $\\forall x\\in \\mathbf{R}^{n}$ with $x\\ne0$,\n",
    "\n",
    "> $x^T(M+N)x = x^TMx+x^TNx$\n",
    "\n",
    "> For M is positive semidefinite and N is positive definite,\n",
    "\n",
    "> $x^TMx\\geq0$ and $x^TNx>0$,\n",
    "\n",
    "> So $x^T(M+N)x>0$\n",
    "\n",
    "> $x^T(M+N)x$ is positive definite and invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Kernel Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a set of vectors $S = {x_1, \\cdots, x_m}$. Let $X$ denote the matrix whose rows are these vectors. Form the Gram matrix $K = XX^T$. Show that knowing $K$ is equivalent to knowing the set of pairwise distances among the vectors in $S$ as well as the vector lengths. [Hint: The distance between $x$ and $y$ is given by $d(x, y) = ||x − y||$, and the norm of a vector x is defined as $||x||=\\sqrt{x^Tx}$.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $K = XX^T$\n",
    "\n",
    "> $=\\begin{pmatrix}\n",
    "x_1^Tx_1&x_1^Tx_2&\\cdots&x_1^Tx_m\\\\\n",
    "\\vdots&\\vdots&\\cdots&\\vdots\\\\\n",
    "x_m^Tx_1&x_m^Tx_2&\\cdots&x_m^Tx_m\\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "> Suppose we want to know the distance between $x_i, x_j$ from $S$,\n",
    "\n",
    "> $d(x_i,x_j) = \\sqrt{(x_i-x_j)^T(x_i-x_j)}$\n",
    "\n",
    "> $=\\sqrt{x_i^Tx_i+x_j^Tx_j-2*x_i^Tx_j}$\n",
    "\n",
    "> $=\\sqrt{k_{ii}+k_{jj}-2k_{ij}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. Show that for $w$ to be a minimizer of $J(w)$, we must have $X^TXw + \\lambda Iw=X^Ty$. Show that the minimizer of $J(w)$ is $w = (X^TX+\\lambda I)^{-1}X^Ty$. Justify that the matrix $X^TX+\\lambda I$ is invertible, for $\\lambda > 0$. (The last part should follow easily from the earlier exercises on psd and spd matrices.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $J(w)=||Xw-y||^2+\\lambda||w||^2$\n",
    "\n",
    "> $\\partial J(w)=2X^TXw-2X^Ty+\\lambda Iw$\n",
    "\n",
    "> To minimize $J(w)$, $\\partial J(w)=0$\n",
    "\n",
    "> So $X^TXw + \\lambda Iw=X^Ty$,\n",
    "\n",
    "> and the minimizer $w = (X^TX+\\lambda I)^{-1}X^Ty$.\n",
    "\n",
    "> $\\forall \\alpha\\in\\mathbf R^n$, with $\\alpha\\ne0,$\n",
    "\n",
    "> $\\alpha^T(X^TX+\\lambda I)\\alpha$\n",
    "\n",
    "> $=\\alpha^TX^TX\\alpha+\\lambda\\alpha^T\\alpha$\n",
    "\n",
    "> As $\\alpha^TX^TX\\alpha\\geq0$ and $\\lambda\\alpha^T\\alpha>0$,\n",
    "\n",
    "> Therefore, $X^TX+\\lambda I$ is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. Rewrite $X^TXw+\\lambda Iw=X^Ty$ as $w=\\frac{1}{\\lambda}(X^Ty-X^TXw)$. Based on this, show that we can write $w=X^T\\alpha$ for some $\\alpha$, and give an expression for $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $w=X^T\\alpha$\n",
    "\n",
    "> where, $\\alpha=\\frac{1}{\\lambda}(y-Xw)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3. Based on the fact that $w=X^T\\alpha$, explain why we say w is \"in the span of the data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $w=X^T\\alpha$, which means w is the linear combination of $x$, and this is why we say w as \"in the span of the data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4. Show that $\\alpha=(\\lambda I + XX^T)^{−1}y$. Note that $XX^T$ is the kernel matrix for the standard vector\n",
    "dot product. (Hint: Replace $w$ by $X^T\\alpha$ in the expression for $\\alpha$, and then solve for $\\alpha$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $\\alpha = \\frac{1}{\\lambda}(Y-Xw)$\n",
    "\n",
    "> $\\lambda I\\alpha=Y-XX^T\\alpha$\n",
    "\n",
    "> $\\alpha = (\\lambda I+XX^T)^{-1}y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5. Give a kernelized expression for the $Xw$, the predicted values on the training points. (Hint: Replace $w$ by $X^T\\alpha$ and $\\alpha$ by its expression in terms of the kernel matrix $XX^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $Xw = XX^T\\alpha$\n",
    "\n",
    "> $=XX^T(\\lambda I+XX^T)^{-1}y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6. Give an expression for the prediction $f(x)=x^Tw^∗$ for a new point $x$, not in the training set. The expression should only involve $x$ via inner products with other $x$’s. [Hint: It is often convenient to define the column vector $$k_x=\\begin{pmatrix}\n",
    "x^Tx_1\\\\\n",
    "\\vdots\\\\\n",
    "x^Tx_n\\\\\n",
    "\\end{pmatrix}$$\n",
    "to simplify the expression.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $w^*=X^T(\\lambda I+XX^T)^{-1}y$\n",
    "\n",
    "> let $$k_x=\\begin{pmatrix}\n",
    "x^Tx_1\\\\\n",
    "\\vdots\\\\\n",
    "x^Tx_n\\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "> Then $f(x)=X^Tw^*=k_x(\\lambda I+XX^T)^{-1}y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Building Trees by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.1. What is the first split for a binary classification tree on this data, using the Gini index? Work this out “by hand”, and show your calculations. [Hint: This should only require calculating 6 weighted impurity measures.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> 1.Consider Spots as the split, $R_1$ for Spots=\"N\" and $R_2$ for Spots=\"Y\",\n",
    "\n",
    "|$N_1$ | 4 |\n",
    "|------|------|\n",
    "|$\\hat{p_1}$ | 0 |\n",
    "|$Q_1$ | 0 |\n",
    "|$N_2$ | 7 |\n",
    "|$\\hat{p_2}$ | $\\frac{5}{7}$  |\n",
    "|$Q_2$ | $\\frac{20}{49}$ |\n",
    "|$N_1Q_1+N_2Q_2$ | $\\frac{20}{7}=2.857$ |\n",
    "\n",
    "> 2.Consider Color as the split, $R_1$ for Color=\"White\" and $R_2$ for Color=\"Brown\",\n",
    "\n",
    "\n",
    "|$N_1$           | 5 |\n",
    "|------          |------|\n",
    "|$\\hat{p_1}$     | $\\frac{2}{5}$ |\n",
    "|$Q_1$           | $\\frac{12}{25}$ |\n",
    "|$N_2$           | 6 |\n",
    "|$\\hat{p_2}$     | $\\frac{1}{2}$  |\n",
    "|$Q_2$           | $\\frac{1}{2}$ |\n",
    "|$N_1Q_1+N_2Q_2$ | $\\frac{27}{5}=5.4$ |\n",
    "\n",
    "> 3.Consider Size as the split, $R_1$ for Size$\\leq1$ and $R_2$ for Size>1,\n",
    "\n",
    "\n",
    "|$N_1$           | 3 |\n",
    "|------          |------|\n",
    "|$\\hat{p_1}$     | $\\frac{2}{3}$ |\n",
    "|$Q_1$           | $\\frac{4}{9}$ |\n",
    "|$N_2$           | 8 |\n",
    "|$\\hat{p_2}$     | $\\frac{3}{8}$  |\n",
    "|$Q_2$           | $\\frac{15}{32}$ |\n",
    "|$N_1Q_1+N_2Q_2$ | $\\frac{61}{12}=5.083$ |\n",
    "\n",
    "> 4.Consider Size as the split, $R_1$ for Size$\\leq2$ and $R_2$ for Size>2,\n",
    "\n",
    "\n",
    "|$N_1$           | 5 |\n",
    "|------          |------|\n",
    "|$\\hat{p_1}$     | $\\frac{2}{5}$ |\n",
    "|$Q_1$           | $\\frac{12}{25}$ |\n",
    "|$N_2$           | 6 |\n",
    "|$\\hat{p_2}$     | $\\frac{1}{2}$  |\n",
    "|$Q_2$           | $\\frac{1}{2}$ |\n",
    "|$N_1Q_1+N_2Q_2$ | $\\frac{27}{5}=5.4$ |\n",
    "\n",
    "> 5.Consider Size as the split, $R_1$ for Size$\\leq3$ and $R_2$ for Size>3,\n",
    "\n",
    "\n",
    "|$N_1$           | 6 |\n",
    "|------          |------|\n",
    "|$\\hat{p_1}$     | $\\frac{1}{3}$ |\n",
    "|$Q_1$           | $\\frac{4}{9}$ |\n",
    "|$N_2$           | 5 |\n",
    "|$\\hat{p_2}$     | $\\frac{3}{5}$  |\n",
    "|$Q_2$           | $\\frac{12}{25}$ |\n",
    "|$N_1Q_1+N_2Q_2$ | $\\frac{76}{15}=5.067$ |\n",
    "\n",
    "> 6.Consider Size as the split, $R_1$ for Size$\\leq4$ and $R_2$ for Size>4,\n",
    "\n",
    "\n",
    "|$N_1$           | 9 |\n",
    "|------          |------|\n",
    "|$\\hat{p_1}$     | $\\frac{4}{9}$ |\n",
    "|$Q_1$           | $\\frac{40}{81}$ |\n",
    "|$N_2$           | 2 |\n",
    "|$\\hat{p_2}$     | $\\frac{1}{2}$  |\n",
    "|$Q_2$           | $\\frac{1}{2}$ |\n",
    "|$N_1Q_1+N_2Q_2$ | $\\frac{49}{9}=5.444$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.2. The first split partitions the data into two parts. Make another split so that the space is partitioned into 3 regions. Determine the predicted “probability of poisonous” for each of those regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> Take the split as, \n",
    "\n",
    "> $R_1$: Spots=\"N\"\n",
    "\n",
    "> $R_2$: Spots=\"Y\" and Size$<4$\n",
    "\n",
    "> $R_3$: Spots=\"Y\" and Size$\\geq4$\n",
    "\n",
    "|$N_1$           | 4 |\n",
    "|-------         |-------|\n",
    "|$\\hat{p_1}$     | 0 |\n",
    "|$Q_1$           | 0 |\n",
    "|$N_2$           | 4 |\n",
    "|$\\hat{p_2}$     | $\\frac{1}{2}$ |\n",
    "|$Q_2$           | $\\frac{1}{2}$ |\n",
    "|$N_3$           | 3 |\n",
    "|$\\hat{p_3}$     | 0  |\n",
    "|$Q_3$           | 0 |\n",
    "|$N_1Q_1+N_2Q_2+N_3Q_3$ | 2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.3. Suppose we build a binary tree on the dataset given below using the Gini criterion and we build it so deep that all terminal nodes are either pure or cannot be split further. (To think about: How could we have a node that is not pure, but cannot be split further?) What would the training error be, given as a percentage? Why? [Hint: You can do this by inspection, without any significant calculations.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> At some node splits, the data may not be linearly separable. And the trainning error(in percentage) should be 18.18%(2 in 11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Investigating Impurity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a data set with 400 data points from class $C_1$ and 400 data points from class $C_2$. Suppose that a tree model $A$ splits these into (300, 100) at the first leaf node and (100, 300) at the second leaf node, where $(n, m)$ denotes that $n$ points are assigned to $C_1$ and $m$ points are assigned to $C_2$. Similarly, suppose that a second tree model $B$ splits them into (200, 400) and (200, 0). Show that the misclassification rates for the two trees are equal, but that the cross-entropy and Gini impurity measures are both lower for tree $B$ than for tree $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> Misclassification Rate\n",
    "\n",
    ">> Tree A = $1-\\frac{200+400}{800}=0.25$ \n",
    "\n",
    ">> Tree B = $1-\\frac{300+300}{800}=0.25$\n",
    "\n",
    "> Cross-entropy \n",
    "\n",
    ">> Tree A = $-\\frac{3}{4}log\\frac{3}{4}-\\frac{3}{4}log\\frac{3}{4}=0.432$\n",
    "\n",
    ">> Tree B = $-\\frac{2}{3}log\\frac{2}{3}+0= 0.2703$\n",
    "\n",
    "> Gini Impurity Measure \n",
    "\n",
    ">> Tree A = $\\frac{1}{4}\\times\\frac{3}{4}+\\frac{3}{4}\\times\\frac{1}{4}=0.375$\n",
    "\n",
    ">> Tree B = $\\frac{2}{3}\\times\\frac{1}{3}=0.222$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7  Representer Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $||x||^2=||m_0||^2+||x-m_0||^2$\n",
    "\n",
    "> $||m_0||=||x|| \\Rightarrow ||x-m_0||^2=0 \\Rightarrow x-m_0=0 \\Rightarrow x=m_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> If $||w^*||=||w||$, we will have $w=w^*$\n",
    "\n",
    "> If $||w^*||<||w||$, since R is strictly increasing, $R(||w||)<R(||w^*||)$\n",
    "\n",
    "> From the proof in the lecture, we have \n",
    "\n",
    "> $L(\\langle w^*, \\phi(x_1)\\rangle,\\cdots,\\langle w^*, \\phi(x_n)\\rangle)=L(\\langle w, \\phi(x_1)\\rangle,\\cdots,\\langle w, \\phi(x_n)\\rangle)$\n",
    "\n",
    "> Thus, $J(w)<J(w^*)$, which makes a controdiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $\\forall x,y\\in \\mathcal H$, let $J_2=L(\\langle w, \\phi(x_1)\\rangle,\\cdots,\\langle w, \\phi(x_n)\\rangle)$\n",
    "\n",
    "> $\\forall\\lambda\\in[0,1]$\n",
    "\n",
    "> $J_2(\\lambda x+(1-\\lambda)y)$\n",
    "\n",
    "> $=L(\\langle \\lambda x+(1-\\lambda)y, \\phi(x_1)\\rangle,\\cdots,\\langle \\lambda x+(1-\\lambda)y, \\phi(x_n)\\rangle)$\n",
    "\n",
    "> $=\\lambda J_2(x)+(1-\\lambda)J_2(y)$\n",
    "\n",
    "> Thus $J_2$ is a convex problem of $w$, which also means $J(w)$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Novelty Detection \n",
    "\n",
    "A novelty detection algorithm can be based on an algorithm that finds the smallest possible\n",
    "sphere containing the data in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.1 Let $\\phi:X\\to H$ be our feature map, mapping elements of the input space into our “feature space” $H$, which is a Hilbert space (and thus has an inner product). Formulate the novelty detection algorithm described above as an optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> We are going to solve the problem of finding the smallest possible sphere by finding a sphere with center $c$ and radius $r$ that the sphere contains all the data points and has the smallest $r$. The optimization problem is,\n",
    "\n",
    "> $$min_{c,r}r^2$$\n",
    "\n",
    "> $$s.t. ||\\phi(x_i)-c||^2\\leq r^2 ,i=1,2,\\cdots,n.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.2 Give the Lagrangian for this problem, and write an equivalent, unconstrained “inf sup” version of the optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> The Lagrangian for this problem is,\n",
    "\n",
    "> $L(r,c,\\alpha)=r^2+\\sum\\limits_{i=1}^n\\alpha_i(||\\phi(x_i)-c||^2-r^2)$\n",
    "\n",
    "> $=inf_{r,c}sup_{\\alpha\\succeq0}[r^2+\\sum\\limits_{i=1}^{n}\\alpha_i(||\\phi(x_i)−c||^2-r^2)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.3 Show that we have strong duality and thus we will have an equivalent optimization problem if we swap the inf and the sup. [Hint: Use Slater’s qualification conditions.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> The Lagrangian problem is convex. Choose origin as the center $c$ and $r=max(\\phi(x_i))+\\epsilon$, there $\\exists x, f(x)<0$. The problem is feasible, so it has strong duality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.4 Solve the inner minimization problem and give the dual optimization problem. [Note: You may find it convenient to define the kernel function $k(x_i, x_j ) = \\langle\\phi(x_i), \\phi(x_j )\\rangle$ and to write your final problem in terms of the corresponding kernel matrix $K$ to simplify notation.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> The dual optimization problem is $$d^*=sup_{\\alpha\\succeq0}inf_{c,r}[r^2+\\sum\\limits_{i=1}^{n}\\alpha_i(||\\phi(x_i)-c||^2-r^2)]$$\n",
    "\n",
    "> Taking derivate w.r.t. c and r for the Lagrangian and setting to zero gives us\n",
    "\n",
    "> $$\\sum\\limits_{i=1}^{n}\\alpha_i=1$$\n",
    "\n",
    "> and $$c=\\sum\\limits_{i=1}^{n}\\alpha_i\\phi(x_i)$$\n",
    "\n",
    "> Substituting back to the dual problem, we get\n",
    "\n",
    "> $$d^*=sup_{\\alpha\\succeq0}(\\sum\\limits_{i=1}^{n}\\alpha_ik(x_i,x_i)-\\sum\\limits_{i,j=1}^{n}\\alpha_i\\alpha_jk(x_i,x_j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.5 Write an expression for the optimal sphere in terms of the solution to the dual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $$max_{\\alpha\\in\\mathbf R^n}\\sum\\limits_{i=1}^{n}\\alpha_ik(x_i,x_i)-\\sum\\limits_{i,j=1}^{n}\\alpha_i\\alpha_jk(x_i,x_j)$$\n",
    "\n",
    "> $$s.t. \\qquad \\alpha_i\\geq0,\\qquad \\forall i=1,\\cdots,n $$\n",
    "\n",
    "> $$\\sum\\limits_{i=1}^{n}\\alpha_i=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.6 Write down the complementary slackness conditions for this problem, and characterize the points that are the “support vectors”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> This problem has strong duality which is sufficient for the complementary slackness conditions. The support vectors are the $x_i$ for which $\\alpha_i$ is non-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.7 Briefly explain how you would apply this algorithm in practice to detect “novel” instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> When a new data is observed, it is flagged as \"novel\" if it lies outside of the sphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.8 [More Optional] Redo this problem allowing some of the data to lie outside of the sphere,\n",
    "where the number of points outside the sphere can be increased or decreased by adjusting a\n",
    "parameter. (Hint: Use slack variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $$min_{c,r}r^2$$\n",
    "\n",
    "> $$s.t.\\qquad ||\\phi(x_i)-c||^2-r^2\\leq \\xi_i, \\qquad i=1,2,\\cdots,n.$$\n",
    "\n",
    "> $$\\sum\\limits_{i=1}^{n}\\xi_i<k$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
