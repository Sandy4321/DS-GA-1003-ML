{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev1\"><a href=\"#Convex-Surrogate-Loss-Functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Convex Surrogate Loss Functions</a></div><div class=\"lev2\"><a href=\"#Hinge-loss-is-a-convex-surrogate-for-0/1-loss-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Hinge loss is a convex surrogate for 0/1 loss</a></div><div class=\"lev2\"><a href=\"#Multiclass-Hinge-Loss-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Multiclass Hinge Loss</a></div><div class=\"lev1\"><a href=\"#Hinge-Loss-is-a-Special-Case-of-Generalized-Hinge-Loss-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Hinge Loss is a Special Case of Generalized Hinge Loss</a></div><div class=\"lev1\"><a href=\"#Another-Formulation-of-Generalized-Hinge-Loss-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Another Formulation of Generalized Hinge Loss</a></div><div class=\"lev2\"><a href=\"#Question-1-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Question 1</a></div><div class=\"lev2\"><a href=\"#Question-2-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Question 2</a></div><div class=\"lev2\"><a href=\"#Question-3-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Question 3</a></div><div class=\"lev1\"><a href=\"#SGD-for-Multiclass-SVM-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>SGD for Multiclass SVM</a></div><div class=\"lev2\"><a href=\"#Question-1-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Question 1</a></div><div class=\"lev2\"><a href=\"#Question-2-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Question 2</a></div><div class=\"lev2\"><a href=\"#Question-3-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Question 3</a></div><div class=\"lev2\"><a href=\"#Question-4-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Question 4</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DS-GA 1003: Machine Learning and Computational Statistics \n",
    "\n",
    "Homework 6: Generalized Hinge Loss and Multiclass SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex Surrogate Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hinge loss is a convex surrogate for 0/1 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) For any example $(x,y)\\in X\\times\\{−1, 1\\}$, show that $1(y\\neq sign(f(x))\\leq max\\{0,1 −yf(x)\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> If $y\\neq sign(f(x))$, $1-yf(x)>1$, and $lhs=1<rhs$,\n",
    "\n",
    "> If $y=sign(f(x))$, $1-yf(x)<1$, and $lhs=0=rhs$\n",
    "\n",
    "> QED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Show that the hinge loss $max\\{0,1-m\\}$ is a convex function of the margin $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $f_1(x)=0$, $f_2(x)=1-x$ are convex, and their pointwise maximum $f(x)=max\\{0,1-x\\}$ is also convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Suppose our prediction score functions are given by $f_w(x)=w^Tx$. The hinge loss of $f_w$ on any example $(x,y)$ is then $max\\{0,1−yw^Tx\\}$. Show that this is a convex function of w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> For $f_{w,1}=1-yw^Tx$, $\\forall 0\\leq t\\leq 1$\n",
    "\n",
    "> $[tf_{w,1}(w_1)+(1-t)f_{w,1}(w_2)]-f_{w,1}(tw_1+(1-t)w_2)$\n",
    "\n",
    "> $= 1-yw_2^Tx-t(yw_1^Tx+yw_2^Tx)-[1-y[tw_1+(1-t)w_2)]^Tx]$\n",
    "\n",
    "> $=0$\n",
    "\n",
    "> Which means $f_{w,1}$ is a convex functio of w, then the hinge loss $max\\{0,1-yw^Tx\\}$ is also a convex funtion of w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Suppose we have chosen an $h\\in \\mathcal H$, from which we get $f(x)=argmax_{y\\in\\mathcal Y}h(x,y)$. Justify that for any $x\\in X$ and $y\\in Y$, we have $h(x,y)\\leq h(x,f(x))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> For any $x\\in X$ and $y\\in Y$,\n",
    "\n",
    "> $h(x,f(x))=max_{y\\in \\mathcal Y}(h(x,y))\\geq(h(x,y))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(2) Justify the following two inequalities:\n",
    "\n",
    "$$\\begin{array}{lll}\n",
    "\\Delta(y,f(x)) & \\le & \\Delta(y,f(x))+h(x,f(x))-h(x,y)\\\\\n",
    "& \\le & \\max\\limits_{y'\\in\\mathcal Y}[\\Delta(y,y'))+h(x,y')-h(x,y)]\\\\\n",
    "\\end{array}$$\n",
    "\n",
    "The RHS of the last expression is called the __generalized hinge loss__: \n",
    "$$\\ell(h,(x,y)) = \\max\\limits_{y_0\\in\\mathcal Y}[\\Delta(y,y_0))+h(x,y_0)−h(x,y)]$$\n",
    "\n",
    "We have shown that for any $x\\in\\mathcal X,y\\in\\mathcal Y,h\\in\\mathcal H$ we have \n",
    "$$\\ell(h,(x,y))\\geq\\Delta(y,f(x)),$$\n",
    "where, as usual, $f(x)=arg\\max\\limits_{y\\in\\mathcal Y}h(x,y)$. [You should think about why we cannot write\n",
    "the generalized hinge loss as $\\ell(f,(x,y))$.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $h(x,f(x))=\\max\\limits_{y'\\in\\mathcal Y}(x,y')$\n",
    "\n",
    "> $h(x,f(x))-h(x,y)\\geq0$\n",
    "\n",
    "> $\\therefore \\Delta(y,f(x))+h(x,f(x))-h(x,y)\\geq\\Delta(y,f(x))$\n",
    "\n",
    "> And, $\\Delta(y,f(x))+h(x,f(x))-h(x,y)\\leq\\max\\limits_{f\\in\\mathcal F}[\\Delta(y,f(x))+h(x,f(x))-h(x,y)]$\n",
    "\n",
    "> $\\leq\\max\\limits_{y'\\in\\mathcal Y}[\\Delta(y,y'))+h(x,y')-h(x,y)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) We now introduce a specific base hypothesis space H of linear functions. Consider a class sensitive\n",
    "feature mapping $\\Psi:X\\times Y\\mapsto \\mathbf R^d$, and $\\mathcal H=\\{h_w(x,y)=\\langle w,\\Psi(x,y) \\rangle|w\\in \\mathbf R^d\\}$.\n",
    "Show that we can write the generalized hinge loss for $h_w(x,y)$ on example $(x_i,y_i)$ as\n",
    "$$\\ell(h_w,(x_i,y_i))=\\max\\limits_{y \\in\\mathcal Y}[\\Delta(y_i,y)+\\langle w,\\Psi(x_i,y)-\\Psi(x_i,y_i)\\rangle].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> For inner product in Hilbert space, the linearity property of that ensures,\n",
    "\n",
    "> $\\langle w, \\Psi(x_i,y)-\\Psi(x_i,y_i)\\rangle$\n",
    "\n",
    "> $=\\langle w, \\Psi(x_i,y)\\rangle-\\langle w, \\Psi(x_i,y_i)\\rangle$\n",
    "\n",
    "> $\\therefore \\ell(h_w,(x_i,y_i))=\\max\\limits_{y \\in\\mathcal Y}[\\Delta(y_i,y)+\\langle w,\\Psi(x_i,y)-\\Psi(x_i,y_i)\\rangle]$\n",
    "\n",
    "> $=\\max\\limits_{y \\in\\mathcal Y}[\\Delta(y_i,y)+h(x_i,y)-h(x_i,y_i)]$\n",
    "\n",
    "> $=\\max\\limits_{y \\in\\mathcal Y}[\\Delta(y_i,y)+\\langle w, \\Psi(x_i,y)\\rangle-\\langle w, \\Psi(x_i,y_i)\\rangle]$\n",
    "\n",
    "> $=\\max\\limits_{y \\in\\mathcal Y}[\\Delta(y_i,y)+\\langle w,\\Psi(x_i,y)-\\Psi(x_i,y_i)\\rangle]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) We will now show that the generalized hinge loss $\\ell(h_w,(x_i,y_i))$ is a convex function of $w$.\n",
    "\n",
    "Justify each of the following steps.\n",
    "\n",
    "(a) The expression $\\Delta(y_i,y)+\\langle w,\\Psi(x_i,y)−\\Psi(x_i,y_i\\rangle$ is an affine function of $w$.\n",
    "\n",
    "(b) The expression $\\max\\limits_{y\\in\\mathcal Y}[\\Delta(y_i,y)+\\langle w,\\Psi(x_i,y)-\\Psi(x_i,y_i)\\rangle]$ is a convex function of w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> (a)\n",
    "\n",
    "> $\\Phi(w+w'-w'')=\\Delta(y_i,y)+\\langle w+w'-w'',\\Psi(x_i,y)−\\Psi(x_i,y_i\\rangle$\n",
    "\n",
    "> $=[\\Delta(y_i,y)+\\langle w,\\Psi(x_i,y)−\\Psi(x_i,y_i\\rangle]+[\\Delta(y_i,y)+\\langle w',\\Psi(x_i,y)−\\Psi(x_i,y_i\\rangle]-[\\Delta(y_i,y)+\\langle w'',\\Psi(x_i,y)−\\Psi(x_i,y_i\\rangle]$\n",
    "\n",
    "> $=\\Phi(w)+\\Phi(w')+\\Phi(w'')$\n",
    "\n",
    "> Which proofs that $\\Phi(w)$ is an affine function.\n",
    "\n",
    "> (b)\n",
    "\n",
    "> $\\forall y\\in \\mathcal Y, \\Delta(y_i,y)+\\langle w,\\Psi(x_i,y)−\\Psi(x_i,y_i\\rangle$ is affine and convex.\n",
    "\n",
    "> $\\therefore \\max\\limits_{y\\in\\mathcal Y}[\\Delta(y_i,y)+\\langle w,\\Psi(x_i,y)-\\Psi(x_i,y_i)\\rangle]$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Conclude that $\\ell(h_w,(x_i,y_i))$ is a convex surrogate for $\\Delta(y_i, f_w(x_i))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $\\because \\ell(h_w,(x_i,y_i))\\geq\\Delta(y,f(x))$\n",
    "\n",
    "> $\\ell(h_w,(x_i,y_i))$ is convex,\n",
    "\n",
    "> $\\therefore \\ell(h_w,(x_i,y_i))$ is the convex surrogate for $\\Delta(y,f(x))$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hinge Loss is a Special Case of Generalized Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Y=\\{−1,1\\}$. Let $\\Delta(y,\\hat y)=1(y\\neq\\hat y)$. If $g(x)$ is the score function in our binary classification\n",
    "setting, then define our compatibility function as\n",
    "$$h(x,1) = g(x)/2$$\n",
    "$$h(x,−1) = −g(x)/2.$$\n",
    "Show that for this choice of $h$, the multiclass hinge loss reduces to hinge loss:\n",
    "$\\ell(h,(x,y))=\\max\\limits_{y_0\\in\\mathcal Y}[\\Delta(y,y_0))+h(x,y_0)−h(x,y)]=max\\{0,1−yg(x)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> If $y=y_0$, \n",
    "\n",
    "> $\\Delta(y,y_0))+h(x,y_0)−h(x,y)=1(y\\neq y_0)+g(y_0)-g(y)$\n",
    "\n",
    "> $=0$\n",
    "\n",
    "> If $y\\neq y_0$, \n",
    "\n",
    "> $\\Delta(y,y_0))+h(x,y_0)−h(x,y)=1(y\\neq y_0)+g(y_0)-g(y)$\n",
    "\n",
    "> $=1-yg(x)$ \n",
    "\n",
    "> $\\therefore, \\ell(h(x,y))=\\max\\{0,1-yg(x)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Formulation of Generalized Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Show that $\\ell(h,(x_i,y_i))=\\max\\limits_{y'\\in\\mathcal Y}[\\Delta(y_i,y'))− m_{i,y'}(h)]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> The generalized hinge loss is\n",
    "\n",
    "> $\\ell(h,(x_i,y_i))=\\max\\limits_{y'\\in\\mathcal Y}[\\Delta(y_i,y')+h(x_i,y')-h(x_i,y_i)]$\n",
    "\n",
    "> $=\\max\\limits_{y'\\in\\mathcal Y}[\\Delta(y_i,y'))− m_{i,y'}(h)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Suppose $\\Delta(y,y_0)\\geq0$ for all $y,y_0\\in\\mathcal Y$. Show that for any example $(x_i,y_i)$ and any score function $h$, the multiclass hinge loss we gave in lecture and the generalized hinge loss presened above are equivalent, in the sense that \n",
    "$$\\max\\limits_{y\\in\\mathcal Y}[(\\Delta(y_i,y)−m_i,y(h)))_+]=\\max\\limits_{y\\in\\mathcal Y}(\\Delta(y_i,y)− m_{i,y'}(h))).$$\n",
    "(Hint: This is easy by piecing together other results we have already attained regarding the relationship between $ell$ and $\\Delta$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $\\Delta(y_i,y)-m_{i,y'}\\geq\\Delta(y_i,y)\\geq0$\n",
    "\n",
    "> $\\max\\limits_{y\\in\\mathcal Y}(\\Delta(y_i,y)− m_{i,y'}(h)))=\\max\\limits_{y\\in\\mathcal Y}[(\\Delta(y_i,y)−m_i,y(h)))_+]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "In the context of the generalized hinge loss, $\\Delta(y,y_0)$ is like the “target margin” between the score for true class $y$ and the score for class $y_0$. Suppose that our prediction function $f$ gets the correct class on $x_i$. That is, $f(x_i) =arg\\max\\limits_{y_0\\in\\mathcal Y}h(x_i,y_0)=y_i$. Furthermore, assume that all of our target margins are reached or exceeded. That is\n",
    "$$m_{i,y}(h)=h(x_i,y_i)−h(x_i,y)\\geq\\Delta(y_i,y),$$\n",
    "for all $y\\neq y_i$. Show that $\\ell(h,(x_i,y_i))=0$ if we assume that $\\Delta(y,y)=0$ for all $y\\in\\mathcal Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $\\ell(h,(x_i,y_i))=\\max\\limits_{y\\in\\mathcal Y}(\\Delta(y_i,y)− m_{i,y'}(h)))$\n",
    "\n",
    "> $=\\max\\limits_{y\\in\\mathcal Y}[\\Delta(y_i,y_i)1(y=y_i),\\Delta(y_i,y)-m_{i,y}(h)1(y\\neq y_i)]$\n",
    "\n",
    "> $=\\Delta(y_i,y_i)$\n",
    "\n",
    "> $=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD for Multiclass SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "For a training set $(x_1,y_1),\\cdots,(x_n,y_n)$, let $J(w)$ be the $\\ell_2$-regularized empirical risk function\n",
    "for the multiclass hinge loss. We can write this as\n",
    "$$J(w) = \\lambda\\|w\\|^2+\\frac{1}{n}\\sum_{i=1}^n\\max\\limits_{y\\in\\mathcal Y}[\\Delta(y_i,y))+\\langle w,\\Psi(x_i,y)−\\Psi(x_i,y_i)\\rangle].$$\n",
    "We will now show that that $J(w)$ is a convex function of $w$. Justify each of the following steps. As we’ve shown it in a previous problem, you may use the fact that $w\\mapsto\\max\\limits_{y\\in\\mathcal Y}[\\Delta(y_i,y))+\\langle w,\\Psi(x_i,y)−\\Psi(x_i,y_i)\\rangle]$ is a convex function.\n",
    "\n",
    "(a) $\\frac{1}{n}\\sum_{i=1}^n\\max\\limits_{y\\in\\mathcal Y}[\\Delta(y_i,y))+\\langle w,\\Psi(x_i,y)−\\Psi(x_i,y_i)\\rangle]$ is a convex function of $w$.\n",
    "\n",
    "(b) $\\|w\\|^2$ is a convex function of $w$.\n",
    "\n",
    "(c) $J(w)$ is a convex function of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> (a) Lets say $w\\mapsto f(w)=\\max\\limits_{y\\in\\mathcal Y}[\\Delta(y_i,y))+\\langle w,\\Psi(x_i,y)−\\Psi(x_i,y_i)\\rangle]$\n",
    "\n",
    "> $f(w)$ is a convex function,\n",
    "\n",
    "> So we can get $$f(tw_1 + (1 - t)w_2)\\leq tf(w_1) + (1 - t)f(w_2)$$\n",
    "\n",
    "> $$\\sum_{i=1}^n f(tw_1 + (1 - t)w_2)\\leq \\sum_{i=1}^n [tf(w_1) + (1 - t)f(w_2)]$$\n",
    "\n",
    "> $$\\sum_{i=1}^n f(tw_1 + (1 - t)w_2)\\leq \\sum_{i=1}^n tf(w_1) + \\sum_{i=1}^n(1 - t)f(w_2)$$\n",
    "\n",
    "> $$\\sum_{i=1}^n f(tw_1 + (1 - t)w_2)\\leq t\\sum_{i=1}^n f(w_1) + (1 - t)\\sum_{i=1}^n f(w_2)$$\n",
    "\n",
    "> So $\\frac{1}{n}\\sum_{i=1}^n\\max\\limits_{y\\in\\mathcal Y}[\\Delta(y_i,y))+\\langle w,\\Psi(x_i,y)−\\Psi(x_i,y_i)\\rangle]$ is convex.\n",
    "\n",
    "> (b) $w\\in\\mathbf R^d$\n",
    "\n",
    "> $\\|w\\|^2=w^Tw$\n",
    "\n",
    "> $\\nabla\\|w\\|^2= 2$ and proofs that it's convex.\n",
    "\n",
    "> (c) Since we have proofed that the sum of two convex function is also convex,\n",
    "\n",
    "> Both $\\lambda_k\\|w_k\\|^2$ and $\\frac{1}{n}\\sum_{i=1}^n\\max\\limits_{y\\in\\mathcal Y}[\\Delta(y_i,y))+\\langle w,\\Psi(x_i,y)−\\Psi(x_i,y_i)\\rangle]$ are convex,\n",
    "\n",
    "> $J(w)$ is a convex function of w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Since $J(w)$ is convex, it has a subgradient at every point. Give an expression for a subgradient of $J(w)$. You may use any standard results about subgradients, including the result from an earlier homework about subgradients of the pointwise maxima of functions. (Hint: It may be helpful to refer to $\\hat y=arg\\max\\limits_{y\\in\\mathcal Y}[\\Delta(y_i,y)+\\langle w,\\Psi(x_i,y)−\\Psi(x_i,y_i)\\rangle]$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $J(w) = \\lambda\\|w\\|^2+\\frac{1}{n}\\sum_{i=1}^n\\max\\limits_{y\\in\\mathcal Y}[\\Delta(y_i,y))+\\langle w,\\Psi(x_i,y)−\\Psi(x_i,y_i)\\rangle]$\n",
    "\n",
    "> $\\partial J(w)=2\\lambda w+\\partial[\\frac{1}{n}\\sum_{i=1}^n\\Delta(y_i,\\hat y)+\\langle w,\\Psi(x_i,\\hat y) \\rangle-\\langle w,\\Psi(x_i,y_i) \\rangle)]$\n",
    "\n",
    "> $=2\\lambda w+\\frac{1}{n}\\sum_{i=1}^n(\\Psi(x_i,\\hat y)-\\Psi(x_i,y_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Give an expression the stochastic subgradient based on the point $(x_i, y_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $w_t = w_{t-1}-\\eta_t\\nabla_w J_i(w_{t-1})$\n",
    "\n",
    "> $=w_{t-1}-\\eta_t(2\\lambda w_{t-1}+\\frac{1}{n}\\sum_{i=1}^n(\\Psi(x_i,\\hat y)-\\Psi(x_i,y_i)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Give an expression for a minibatch subgradient, based on the points $(x_i, y_i),\\cdots ,(x_{i+m−1}, y_{i+m−1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ANSWER__\n",
    "\n",
    "> $w_t = w_{t-1}-\\eta_t\\nabla_w J_i(w_{t-1})$\n",
    "\n",
    "> $=w_{t-1}-\\eta_t(2\\lambda w_{t-1}+\\frac{1}{n}\\sum_{j=1}^m\\sum_{i=1}^n(\\Psi(x_{i+j-1},\\hat y)-\\Psi(x_{i+j-1},y_{i+j-1})))$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "toc_cell": true,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
